{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting symbols from image using `OpenCV`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import cv2\n",
    "import numpy as np\n",
    "from functools import cmp_to_key\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractSymbols(imgOrig, showSteps = False):\n",
    "    debugImgSteps = []\n",
    "    imgGray = cv2.cvtColor(imgOrig,cv2.COLOR_BGR2GRAY)\n",
    "    imgFiltered = cv2.medianBlur(imgGray, 5)\n",
    "    debugImgSteps.append(imgFiltered)\n",
    "    \n",
    "    imgCanny = cv2.Canny(imgFiltered, 50,180)\n",
    "    debugImgSteps.append(imgCanny)\n",
    "\n",
    "    kernel = np.ones((5,5), np.uint8)\n",
    "    imgDilated = cv2.dilate(imgCanny, kernel, iterations=5)\n",
    "    debugImgSteps.append(imgDilated)\n",
    "\n",
    "    contours, _= cv2.findContours(imgDilated, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "    boundingBoxes = []\n",
    "    for contour in contours:\n",
    "        x,y,w,h = cv2.boundingRect(contour)\n",
    "        boundingBoxes.append((x,y,w,h))\n",
    "\n",
    "    global rowsG\n",
    "    rowsG, _, _ = imgOrig.shape\n",
    "    key_leftRightTopBottom = cmp_to_key(leftRightTopBottom)\n",
    "    boundingBoxes = sorted(boundingBoxes, key=key_leftRightTopBottom)\n",
    "\n",
    "    symbols = []\n",
    "    for (i, box) in enumerate(boundingBoxes):\n",
    "        x,y,w,h = box\n",
    "        mathSymbol = imgOrig[y:y+h, x:x+w]\n",
    "        mathSymbol = cv2.cvtColor(mathSymbol, cv2.COLOR_BGR2GRAY) #converting to Gray as tensorflow deals with grayscale or RGB, not BGR\n",
    "        mathSymbol = cv2.resize(mathSymbol, (45,45), interpolation=cv2.INTER_AREA) #to have the same size as trained images in the dataset\n",
    "        debugImgSteps.append(mathSymbol)\n",
    "        mathSymbolF = mathSymbol.astype('float32') #optional: tensorflows deals with float32, not uint8\n",
    "        tf.keras.utils.normalize(mathSymbolF, axis=1)\n",
    "        symbols.append(mathSymbolF)\n",
    "\n",
    "    if showSteps:\n",
    "        dispImages(debugImgSteps)\n",
    "\n",
    "    return symbols\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leftRightTopBottom(tup1, tup2):\n",
    "    x1, y1, _, _ = tup1\n",
    "    x2, y2, _, _ = tup2\n",
    "    rows = rowsG\n",
    "    yRegion1, yRegion2 = -1, -1\n",
    "\n",
    "    for i in range(4):\n",
    "        if y1 < rows/4 + rows*(i/4):\n",
    "            yRegion1 = i\n",
    "            break\n",
    "    else:\n",
    "        if yRegion1 == -1:\n",
    "            yRegion1 = 4\n",
    "\n",
    "    for i in range(4):\n",
    "        if y2 < rows/4 + rows*(i/4):\n",
    "            yRegion2 = i\n",
    "            break\n",
    "    else:\n",
    "        if yRegion2 == -1:\n",
    "            yRegion2 = 4\n",
    "    \n",
    "    if yRegion1 < yRegion2:\n",
    "        return -1\n",
    "    elif yRegion2 < yRegion1:\n",
    "        return 1\n",
    "    elif x1 <= x2:\n",
    "        return -1\n",
    "    else:\n",
    "        return 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dispImages(imgs):\n",
    "    for img in imgs:\n",
    "        cv2.imshow('Image', img)\n",
    "        cv2.waitKey(0)\n",
    "    else:\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('tests/test16.png')\n",
    "symbols = extractSymbols(img, showSteps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dispImages(symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating dictionary that maps folder names to latex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example of folder names in \"mathSymbolsDataset\": <br>\n",
    "<img src=\"guideImages/datasetFolders.png\" width=400 height=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using `r` to make the string `raw` to avoid confusing strings like `\\n` with python's new line <br>\n",
    "however, the values will now have two backslashes (e.g. `\\\\n`), thus, we will later need to replace each `\\\\` with `\\`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {\n",
    "    \"-\": r\"-\",\n",
    "    \"(\": r\"(\",\n",
    "    \")\": r\")\",   \n",
    "    \"+\": r\"+\",\n",
    "    \"=\": r\"=\",\n",
    "    \"0\": r\"0\",\n",
    "    \"1\": r\"1\",\n",
    "    \"2\": r\"2\",\n",
    "    \"3\": r\"3\",\n",
    "    \"4\": r\"4\",\n",
    "    \"5\": r\"5\",\n",
    "    \"6\": r\"6\",\n",
    "    \"7\": r\"7\",\n",
    "    \"8\": r\"8\",\n",
    "    \"9\": r\"9\",\n",
    "    \n",
    "    \"geq\": r\"\\geq\",\n",
    "    \"gt\": r\">\",\n",
    "    \"i\": r\"i\",\n",
    "    \"in\": r\"\\in\",\n",
    "    \"int\": r\"\\int\",\n",
    "    \"j\": r\"j\",\n",
    "\n",
    "    \"leq\": r\"\\le\",\n",
    "\n",
    "    \"lt\": r\"<\",\n",
    "    \n",
    "    \"neq\": r\"\\neq\",\n",
    "    \n",
    "    \"pi\": r\"\\Pi\",\n",
    "    \n",
    "    \"sum\": r\"\\sum\",\n",
    "    \"theta\": r\"\\theta\",\n",
    "    \"times\": r\"\\times\",\n",
    "\n",
    "    \"w\": r\"w\",\n",
    "    \"X\": r\"\\X\",\n",
    "    \"y\": r\"y\",\n",
    "    \"z\": r\"z\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the kaggle [dataset](https://www.kaggle.com/datasets/xainano/handwrittenmathsymbols?resource=download)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "1. create a list of images and another list of labels for each image\n",
    "2. store them in pickle files for easy retrieval when re-running the code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(dataDir):\n",
    "    imgs = []\n",
    "    labels = []\n",
    "    for key, value in dic.items():\n",
    "        path = os.path.join(dataDir, key)\n",
    "        print(path)\n",
    "        for imgName in os.listdir(path):\n",
    "            try:\n",
    "                img = cv2.imread(os.path.join(path, imgName), cv2.COLOR_BGR2GRAY) \n",
    "                imgs.append(img)\n",
    "                labels.append(value)\n",
    "            except Exception as e:\n",
    "                print(e)    \n",
    "    return (imgs, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell is commented as it takes a long time (10min if image RGB, 1min otherwise) to create the pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mathSymbolsDataset/-\n",
      "mathSymbolsDataset/(\n",
      "mathSymbolsDataset/)\n",
      "mathSymbolsDataset/+\n",
      "mathSymbolsDataset/=\n",
      "mathSymbolsDataset/0\n",
      "mathSymbolsDataset/1\n",
      "mathSymbolsDataset/2\n",
      "mathSymbolsDataset/3\n",
      "mathSymbolsDataset/4\n",
      "mathSymbolsDataset/5\n",
      "mathSymbolsDataset/6\n",
      "mathSymbolsDataset/7\n",
      "mathSymbolsDataset/8\n",
      "mathSymbolsDataset/9\n",
      "mathSymbolsDataset/geq\n",
      "mathSymbolsDataset/gt\n",
      "mathSymbolsDataset/i\n",
      "mathSymbolsDataset/in\n",
      "mathSymbolsDataset/int\n",
      "mathSymbolsDataset/j\n",
      "mathSymbolsDataset/leq\n",
      "mathSymbolsDataset/lt\n",
      "mathSymbolsDataset/neq\n",
      "mathSymbolsDataset/pi\n",
      "mathSymbolsDataset/sum\n",
      "mathSymbolsDataset/theta\n",
      "mathSymbolsDataset/times\n",
      "mathSymbolsDataset/w\n",
      "mathSymbolsDataset/X\n",
      "mathSymbolsDataset/y\n",
      "mathSymbolsDataset/z\n"
     ]
    }
   ],
   "source": [
    "imgs, labels = loadData('mathSymbolsDataset/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"x_symbols_reduced.pickle\", 'wb') as f:\n",
    "    pickle.dump(imgs, f)\n",
    "with open(\"y_latex_reduced.pickle\", 'wb') as f:\n",
    "    pickle.dump(labels, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"x_symbols_reduced.pickle\", 'rb') as f:\n",
    "    imgs = pickle.load(f)\n",
    "with open(\"y_latex_reduced.pickle\", 'rb') as f:\n",
    "    labels = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## converting text labels (latex) to numeric codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'(': 0,\n",
       " ')': 1,\n",
       " '+': 2,\n",
       " '-': 3,\n",
       " '0': 4,\n",
       " '1': 5,\n",
       " '2': 6,\n",
       " '3': 7,\n",
       " '4': 8,\n",
       " '5': 9,\n",
       " '6': 10,\n",
       " '7': 11,\n",
       " '8': 12,\n",
       " '9': 13,\n",
       " '<': 14,\n",
       " '=': 15,\n",
       " '>': 16,\n",
       " '\\\\Pi': 17,\n",
       " '\\\\X': 18,\n",
       " '\\\\geq': 19,\n",
       " '\\\\in': 20,\n",
       " '\\\\int': 21,\n",
       " '\\\\le': 22,\n",
       " '\\\\neq': 23,\n",
       " '\\\\sum': 24,\n",
       " '\\\\theta': 25,\n",
       " '\\\\times': 26,\n",
       " 'i': 27,\n",
       " 'j': 28,\n",
       " 'w': 29,\n",
       " 'y': 30,\n",
       " 'z': 31}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latexToNums = {k: v for v, k in enumerate(np.unique(labels))}\n",
    "#this dictionary is to revert the predicted numeric code back to latex: \n",
    "numsToLatex = {v: k for v, k in enumerate(np.unique(labels))}\n",
    "latexToNums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelsNums = [latexToNums[label] for label in labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data into train and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `stratify` is used to split the dataset into train and test sets <br> \n",
    "in a way that preserves the same proportions of examples in each class as observed in the original dataset <br>\n",
    "[(source)](https://machinelearningmastery.com/train-test-split-for-evaluating-machine-learning-algorithms/#:~:text=is%20desirable%20to-,split%20the%20dataset%20into,stratified%20train-test%20split.,-We%20can%20achieve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(imgs, labelsNums, test_size=0.33, stratify=labelsNums, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing image pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tf.keras.utils.normalize(x_train, axis=1) #similar to dividing by 255 (but not equivalent in result)\n",
    "x_test = tf.keras.utils.normalize(x_test, axis=1) #Also, don't know why we are using \"axis=1\" specifically, but that's what's normally used with image normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dispImages([x_train[4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'-'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numsToLatex[y_train[4]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making sure all datasets are `ndarray` not `list`\n",
    "Because `keras` models accept `ndarray`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, numpy.ndarray, list, list)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x_train), type(x_test), type(y_train), type(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, numpy.ndarray)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_train), type(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Sequential vs Functional models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sequential is a linear stack of layers. In other words, the layer `i` is connected only to layers `i-1` and `i+1`\n",
    "* Functional is more dynamic, as each layer can connect to any other layer in the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the images are small in size, and the problem is relatively simple, we'll use a sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Model layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for easier processing: flatten image (e.g. 45x45 will become 1x2025)\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "# 128 nodes are chosen as they are a power of 2 (2^7) which makes computation easier, and the images are not large (45x45) so 128 nodes should suffice\n",
    "# relu is the default activation function to use\n",
    "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\n",
    "# add another layer because if you have one, then you're getting linear relations only between the image's features, while two layers makes it non-linear\n",
    "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\n",
    "# number of classifications == number of stored latex strings == len(latexToNums) == 79\n",
    "# using softmax as it converts the scores to a normalized probability distribution\n",
    "model.add(tf.keras.layers.Dense(len(latexToNums), activation=tf.nn.softmax))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"compiling\" means passing the settings for actually optimizing/training the model we've defined\n",
    "model.compile(optimizer='adam', # same logic as relu, great default optimizer to start with\n",
    "              loss='sparse_categorical_crossentropy', # A neural network doesn't actually attempt to maximize accuracy. It attempts to minimize loss, this loss function is also a great default\n",
    "              metrics=['accuracy']) # ratio between the number of correct predictions to the total number of predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"A good rule of thumb is to start with a value that is 3 times the number of columns in your data.\" <br>\n",
    "[(source)](https://gretel.ai/gretel-synthetics-faqs/how-many-epochs-should-i-train-my-model-with) <br>\n",
    "Therefore, we start by with 45*3 = 135 epochs (i.e. number of passes of the entire training dataset the machine learning algorithm has completed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.fit(x_train, y_train_nums, epochs=135)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Save the model for later use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Technical note: pickle doesn't save models correctly, as it outputs this error when loading the pickle file: <br><br>\n",
    "FileNotFoundError: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for ram://0eb44777-6983-466e-ac15-adfa9d3dae07/variables/variables\n",
    " You may be trying to load on a different device from the computational device. Consider setting the `experimental_io_device` option in `tf.saved_model.LoadOptions` to the io_device such as '/job:localhost'. <br><br>\n",
    " That's why we are using keras's `save()` and `load_model()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save(\"nnModel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"nnModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 45, 45)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symbols[0].reshape(1,45,45).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "symTest = symbols[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    }
   ],
   "source": [
    "print(np.argmax(model.predict(symTest.reshape(1,45,45))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import unique, argmax\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPool2D\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Activation\n",
    "from keras.utils.vis_utils  import plot_model\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3,3), activation  ='relu', input_shape=(45,45,1)))\n",
    "model.add(MaxPool2D((2,2))) \n",
    "#batch normalization, try averagepool\n",
    "#leak\n",
    "model.add(Conv2D(48, (3,3), activation='relu'))\n",
    "model.add(MaxPool2D((2,2)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(2025, activation='relu'))\n",
    "model.add(Dense(79, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1722/1722 - 386s - loss: 0.6359 - accuracy: 0.8275 - val_loss: 0.2793 - val_accuracy: 0.9140 - 386s/epoch - 224ms/step\n",
      "Epoch 2/10\n",
      "1722/1722 - 418s - loss: 0.2386 - accuracy: 0.9268 - val_loss: 0.1760 - val_accuracy: 0.9441 - 418s/epoch - 243ms/step\n",
      "Epoch 3/10\n",
      "1722/1722 - 401s - loss: 0.1689 - accuracy: 0.9461 - val_loss: 0.1280 - val_accuracy: 0.9589 - 401s/epoch - 233ms/step\n",
      "Epoch 4/10\n",
      "1722/1722 - 423s - loss: 0.1312 - accuracy: 0.9569 - val_loss: 0.1148 - val_accuracy: 0.9613 - 423s/epoch - 246ms/step\n",
      "Epoch 5/10\n",
      "1722/1722 - 406s - loss: 0.1091 - accuracy: 0.9634 - val_loss: 0.0922 - val_accuracy: 0.9711 - 406s/epoch - 236ms/step\n",
      "Epoch 6/10\n",
      "1722/1722 - 410s - loss: 0.0934 - accuracy: 0.9684 - val_loss: 0.0835 - val_accuracy: 0.9761 - 410s/epoch - 238ms/step\n",
      "Epoch 7/10\n",
      "1722/1722 - 428s - loss: 0.0828 - accuracy: 0.9721 - val_loss: 0.0740 - val_accuracy: 0.9781 - 428s/epoch - 248ms/step\n",
      "Epoch 8/10\n",
      "1722/1722 - 430s - loss: 0.0739 - accuracy: 0.9752 - val_loss: 0.0722 - val_accuracy: 0.9804 - 430s/epoch - 250ms/step\n",
      "Epoch 9/10\n",
      "1722/1722 - 379s - loss: 0.0676 - accuracy: 0.9773 - val_loss: 0.0653 - val_accuracy: 0.9817 - 379s/epoch - 220ms/step\n",
      "Epoch 10/10\n",
      "1722/1722 - 363s - loss: 0.0623 - accuracy: 0.9791 - val_loss: 0.0603 - val_accuracy: 0.9833 - 363s/epoch - 211ms/step\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer = 'adam', loss='sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "x = model.fit(x_train, y_train_nums, epochs=10, batch_size=128, verbose=2, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save(\"FModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"FModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'argmax' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Farah\\Documents\\Handwritten-M2L\\farah.ipynb Cell 60'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Farah/Documents/Handwritten-M2L/farah.ipynb#ch0000059?line=0'>1</a>\u001b[0m p \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(symbols[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m,\u001b[39m45\u001b[39m,\u001b[39m45\u001b[39m))\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Farah/Documents/Handwritten-M2L/farah.ipynb#ch0000059?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(argmax(p))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'argmax' is not defined"
     ]
    }
   ],
   "source": [
    "p = model.predict(symbols[1].reshape(1,45,45))\n",
    "print(argmax(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train = np.array(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = Sequential()\n",
    "#model.add(Conv2D(64, (3,3), input_shape=x_train.shape))\n",
    "#model.add(Activation(\"relu\"))\n",
    "#model.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "#model.add(Conv2D(64, (3,3)))\n",
    "#model.add(Activation(\"relu\"))\n",
    "#model.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "#model.add(Flatten())\n",
    "#model.add(Dense(64))\n",
    "#model.add(Dense(1))\n",
    "#model.add(Activation('sigmoid'))\n",
    "\n",
    "#model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
    "#model.fit(x_train, y_train_nums, batch_size=128, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(64, (3,3), activation  ='relu', input_shape=(45,45,1)))\n",
    "model.add(MaxPool2D((2,2))) #batch normalization, try averagepool\n",
    "#leak\n",
    "model.add(Conv2D(64, (3,3), activation='relu'))\n",
    "model.add(MaxPool2D((2,2)))\n",
    "\n",
    "model.add(Conv2D(64, (3,3), activation='relu'))\n",
    "model.add(MaxPool2D((2,2)))\n",
    "\n",
    "#model.add(Conv2D(64, (3,3), activation='relu'))\n",
    "#model.add(MaxPool2D((2,2)))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(2025, activation='relu'))\n",
    "#model.add(Dense(2025, activation='relu'))\n",
    "BatchNormalization(axis=1)\n",
    "model.add(Dense(79, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1722/1722 - 536s - loss: 0.9281 - accuracy: 0.7486 - val_loss: 0.3709 - val_accuracy: 0.8863 - 536s/epoch - 311ms/step\n",
      "Epoch 2/10\n",
      "1722/1722 - 546s - loss: 0.4311 - accuracy: 0.8686 - val_loss: 0.2845 - val_accuracy: 0.9116 - 546s/epoch - 317ms/step\n",
      "Epoch 3/10\n",
      "1722/1722 - 517s - loss: 0.3508 - accuracy: 0.8897 - val_loss: 0.2320 - val_accuracy: 0.9252 - 517s/epoch - 300ms/step\n",
      "Epoch 4/10\n",
      "1722/1722 - 496s - loss: 0.3076 - accuracy: 0.9011 - val_loss: 0.2139 - val_accuracy: 0.9291 - 496s/epoch - 288ms/step\n",
      "Epoch 5/10\n",
      "1722/1722 - 531s - loss: 0.2768 - accuracy: 0.9098 - val_loss: 0.1982 - val_accuracy: 0.9345 - 531s/epoch - 309ms/step\n",
      "Epoch 6/10\n",
      "1722/1722 - 531s - loss: 0.2555 - accuracy: 0.9155 - val_loss: 0.1801 - val_accuracy: 0.9393 - 531s/epoch - 308ms/step\n",
      "Epoch 7/10\n",
      "1722/1722 - 535s - loss: 0.2383 - accuracy: 0.9213 - val_loss: 0.1676 - val_accuracy: 0.9434 - 535s/epoch - 311ms/step\n",
      "Epoch 8/10\n",
      "1722/1722 - 496s - loss: 0.2243 - accuracy: 0.9252 - val_loss: 0.1571 - val_accuracy: 0.9496 - 496s/epoch - 288ms/step\n",
      "Epoch 9/10\n",
      "1722/1722 - 470s - loss: 0.2133 - accuracy: 0.9288 - val_loss: 0.1466 - val_accuracy: 0.9514 - 470s/epoch - 273ms/step\n",
      "Epoch 10/10\n",
      "1722/1722 - 457s - loss: 0.2019 - accuracy: 0.9316 - val_loss: 0.1401 - val_accuracy: 0.9557 - 457s/epoch - 265ms/step\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer = 'adam', loss='sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "x = model.fit(x_train, y_train_nums, epochs=10, batch_size=128, verbose=2, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "p = model.predict(symbols[0].reshape(1,45,45))\n",
    "print(argmax(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: F1Model\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"F1Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 9577,\n",
       " 1: 9618,\n",
       " 2: 16825,\n",
       " 3: 22778,\n",
       " 4: 4632,\n",
       " 5: 17768,\n",
       " 6: 17514,\n",
       " 7: 7309,\n",
       " 8: 4955,\n",
       " 9: 2375,\n",
       " 10: 2089,\n",
       " 11: 1949,\n",
       " 12: 2056,\n",
       " 13: 2504,\n",
       " 14: 320,\n",
       " 15: 8780,\n",
       " 16: 173,\n",
       " 17: 1562,\n",
       " 18: 17818,\n",
       " 19: 464,\n",
       " 20: 31,\n",
       " 21: 1837,\n",
       " 22: 652,\n",
       " 23: 374,\n",
       " 24: 1802,\n",
       " 25: 1873,\n",
       " 26: 2178,\n",
       " 27: 3444,\n",
       " 28: 1029,\n",
       " 29: 373,\n",
       " 30: 6258,\n",
       " 31: 3933}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numLabels, counts = np.unique(y_train, return_counts=True)\n",
    "numLabelsToFreq = dict(zip(numLabels, counts))\n",
    "numLabelsToFreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 2.378406599143782,\n",
       " 1: 2.368267831149927,\n",
       " 2: 1.3538187221396731,\n",
       " 3: 1.0,\n",
       " 4: 4.917530224525043,\n",
       " 5: 1.2819675821701937,\n",
       " 6: 1.3005595523581135,\n",
       " 7: 3.1164317964153785,\n",
       " 8: 4.596972754793138,\n",
       " 9: 9.590736842105263,\n",
       " 10: 10.903781713738631,\n",
       " 11: 11.687018984094408,\n",
       " 12: 11.078793774319067,\n",
       " 13: 9.09664536741214,\n",
       " 14: 71.18125,\n",
       " 15: 2.5943052391799544,\n",
       " 16: 131.66473988439307,\n",
       " 17: 14.58258642765685,\n",
       " 18: 1.2783701874508924,\n",
       " 19: 49.09051724137931,\n",
       " 20: 734.7741935483871,\n",
       " 21: 12.399564507348938,\n",
       " 22: 34.93558282208589,\n",
       " 23: 60.903743315508024,\n",
       " 24: 12.640399556048834,\n",
       " 25: 12.161238654564869,\n",
       " 26: 10.45821854912764,\n",
       " 27: 6.6138211382113825,\n",
       " 28: 22.13605442176871,\n",
       " 29: 61.06702412868633,\n",
       " 30: 3.639821029082774,\n",
       " 31: 5.791507754894482}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxlabelImgs = max(numLabelsToFreq.values())\n",
    "labelWeights = {label : maxlabelImgs / float(numImgs) for label, numImgs in numLabelsToFreq.items()}\n",
    "labelWeights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(2025, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(2025, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(2025, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(32, activation=tf.nn.softmax))\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      " 350/5465 [>.............................] - ETA: 5:26 - loss: 14.2311 - accuracy: 0.0012"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\Handwritten-M2L\\farah.ipynb Cell 72'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Handwritten-M2L/farah.ipynb#ch0000101?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(x_train, y_train, epochs\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, class_weight\u001b[39m=\u001b[39;49mlabelWeights)\n",
      "File \u001b[1;32me:\\Handwritten-M2L\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///e%3A/Handwritten-M2L/venv/lib/site-packages/keras/utils/traceback_utils.py?line=61'>62</a>\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     <a href='file:///e%3A/Handwritten-M2L/venv/lib/site-packages/keras/utils/traceback_utils.py?line=62'>63</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> <a href='file:///e%3A/Handwritten-M2L/venv/lib/site-packages/keras/utils/traceback_utils.py?line=63'>64</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     <a href='file:///e%3A/Handwritten-M2L/venv/lib/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     <a href='file:///e%3A/Handwritten-M2L/venv/lib/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32me:\\Handwritten-M2L\\venv\\lib\\site-packages\\keras\\engine\\training.py:1384\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   <a href='file:///e%3A/Handwritten-M2L/venv/lib/site-packages/keras/engine/training.py?line=1376'>1377</a>\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   <a href='file:///e%3A/Handwritten-M2L/venv/lib/site-packages/keras/engine/training.py?line=1377'>1378</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   <a href='file:///e%3A/Handwritten-M2L/venv/lib/site-packages/keras/engine/training.py?line=1378'>1379</a>\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   <a href='file:///e%3A/Handwritten-M2L/venv/lib/site-packages/keras/engine/training.py?line=1379'>1380</a>\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[0;32m   <a href='file:///e%3A/Handwritten-M2L/venv/lib/site-packages/keras/engine/training.py?line=1380'>1381</a>\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m   <a href='file:///e%3A/Handwritten-M2L/venv/lib/site-packages/keras/engine/training.py?line=1381'>1382</a>\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m   <a href='file:///e%3A/Handwritten-M2L/venv/lib/site-packages/keras/engine/training.py?line=1382'>1383</a>\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> <a href='file:///e%3A/Handwritten-M2L/venv/lib/site-packages/keras/engine/training.py?line=1383'>1384</a>\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   <a href='file:///e%3A/Handwritten-M2L/venv/lib/site-packages/keras/engine/training.py?line=1384'>1385</a>\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   <a href='file:///e%3A/Handwritten-M2L/venv/lib/site-packages/keras/engine/training.py?line=1385'>1386</a>\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32me:\\Handwritten-M2L\\venv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///e%3A/Handwritten-M2L/venv/lib/site-packages/tensorflow/python/util/traceback_utils.py?line=147'>148</a>\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    <a href='file:///e%3A/Handwritten-M2L/venv/lib/site-packages/tensorflow/python/util/traceback_utils.py?line=148'>149</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///e%3A/Handwritten-M2L/venv/lib/site-packages/tensorflow/python/util/traceback_utils.py?line=149'>150</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    <a href='file:///e%3A/Handwritten-M2L/venv/lib/site-packages/tensorflow/python/util/traceback_utils.py?line=150'>151</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    <a href='file:///e%3A/Handwritten-M2L/venv/lib/site-packages/tensorflow/python/util/traceback_utils.py?line=151'>152</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32me:\\Handwritten-M2L\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    <a href='file:///e%3A/Handwritten-M2L/venv/lib/site-packages/tensorflow/python/eager/def_function.py?line=911'>912</a>\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///e%3A/Handwritten-M2L/venv/lib/site-packages/tensorflow/python/eager/def_function.py?line=913'>914</a>\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> <a href='file:///e%3A/Handwritten-M2L/venv/lib/site-packages/tensorflow/python/eager/def_function.py?line=914'>915</a>\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    <a href='file:///e%3A/Handwritten-M2L/venv/lib/site-packages/tensorflow/python/eager/def_function.py?line=916'>917</a>\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    <a href='file:///e%3A/Handwritten-M2L/venv/lib/site-packages/tensorflow/python/eager/def_function.py?line=917'>918</a>\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32me:\\Handwritten-M2L\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    <a href='file:///e%3A/Handwritten-M2L/venv/lib/site-packages/tensorflow/python/eager/def_function.py?line=943'>944</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    <a href='file:///e%3A/Handwritten-M2L/venv/lib/site-packages/tensorflow/python/eager/def_function.py?line=944'>945</a>\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    <a href='file:///e%3A/Handwritten-M2L/venv/lib/site-packages/tensorflow/python/eager/def_function.py?line=945'>946</a>\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> <a href='file:///e%3A/Handwritten-M2L/venv/lib/site-packages/tensorflow/python/eager/def_function.py?line=946'>947</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    <a href='file:///e%3A/Handwritten-M2L/venv/lib/site-packages/tensorflow/python/eager/def_function.py?line=947'>948</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///e%3A/Handwritten-M2L/venv/lib/site-packages/tensorflow/python/eager/def_function.py?line=948'>949</a>\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    <a href='file:///e%3A/Handwritten-M2L/venv/lib/site-packages/tensorflow/python/eager/def_function.py?line=949'>950</a>\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    <a href='file:///e%3A/Handwritten-M2L/venv/lib/site-packages/tensorflow/python/eager/def_function.py?line=950'>951</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32me:\\Handwritten-M2L\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2956\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///e%3A/Handwritten-M2L/venv/lib/site-packages/tensorflow/python/eager/function.py?line=2952'>2953</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   <a href='file:///e%3A/Handwritten-M2L/venv/lib/site-packages/tensorflow/python/eager/function.py?line=2953'>2954</a>\u001b[0m   (graph_function,\n\u001b[0;32m   <a href='file:///e%3A/Handwritten-M2L/venv/lib/site-packages/tensorflow/python/eager/function.py?line=2954'>2955</a>\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> <a href='file:///e%3A/Handwritten-M2L/venv/lib/site-packages/tensorflow/python/eager/function.py?line=2955'>2956</a>\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   <a href='file:///e%3A/Handwritten-M2L/venv/lib/site-packages/tensorflow/python/eager/function.py?line=2956'>2957</a>\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32me:\\Handwritten-M2L\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1853\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   <a href='file:///e%3A/Handwritten-M2L/venv/lib/site-packages/tensorflow/python/eager/function.py?line=1848'>1849</a>\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   <a href='file:///e%3A/Handwritten-M2L/venv/lib/site-packages/tensorflow/python/eager/function.py?line=1849'>1850</a>\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   <a href='file:///e%3A/Handwritten-M2L/venv/lib/site-packages/tensorflow/python/eager/function.py?line=1850'>1851</a>\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   <a href='file:///e%3A/Handwritten-M2L/venv/lib/site-packages/tensorflow/python/eager/function.py?line=1851'>1852</a>\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> <a href='file:///e%3A/Handwritten-M2L/venv/lib/site-packages/tensorflow/python/eager/function.py?line=1852'>1853</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   <a href='file:///e%3A/Handwritten-M2L/venv/lib/site-packages/tensorflow/python/eager/function.py?line=1853'>1854</a>\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   <a href='file:///e%3A/Handwritten-M2L/venv/lib/site-packages/tensorflow/python/eager/function.py?line=1854'>1855</a>\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   <a href='file:///e%3A/Handwritten-M2L/venv/lib/site-packages/tensorflow/python/eager/function.py?line=1855'>1856</a>\u001b[0m     args,\n\u001b[0;32m   <a href='file:///e%3A/Handwritten-M2L/venv/lib/site-packages/tensorflow/python/eager/function.py?line=1856'>1857</a>\u001b[0m     possible_gradient_type,\n\u001b[0;32m   <a href='file:///e%3A/Handwritten-M2L/venv/lib/site-packages/tensorflow/python/eager/function.py?line=1857'>1858</a>\u001b[0m     executing_eagerly)\n\u001b[0;32m   <a href='file:///e%3A/Handwritten-M2L/venv/lib/site-packages/tensorflow/python/eager/function.py?line=1858'>1859</a>\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32me:\\Handwritten-M2L\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    <a href='file:///e%3A/Handwritten-M2L/venv/lib/site-packages/tensorflow/python/eager/function.py?line=496'>497</a>\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    <a href='file:///e%3A/Handwritten-M2L/venv/lib/site-packages/tensorflow/python/eager/function.py?line=497'>498</a>\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///e%3A/Handwritten-M2L/venv/lib/site-packages/tensorflow/python/eager/function.py?line=498'>499</a>\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    <a href='file:///e%3A/Handwritten-M2L/venv/lib/site-packages/tensorflow/python/eager/function.py?line=499'>500</a>\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    <a href='file:///e%3A/Handwritten-M2L/venv/lib/site-packages/tensorflow/python/eager/function.py?line=500'>501</a>\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    <a href='file:///e%3A/Handwritten-M2L/venv/lib/site-packages/tensorflow/python/eager/function.py?line=501'>502</a>\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    <a href='file:///e%3A/Handwritten-M2L/venv/lib/site-packages/tensorflow/python/eager/function.py?line=502'>503</a>\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    <a href='file:///e%3A/Handwritten-M2L/venv/lib/site-packages/tensorflow/python/eager/function.py?line=503'>504</a>\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    <a href='file:///e%3A/Handwritten-M2L/venv/lib/site-packages/tensorflow/python/eager/function.py?line=504'>505</a>\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///e%3A/Handwritten-M2L/venv/lib/site-packages/tensorflow/python/eager/function.py?line=505'>506</a>\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    <a href='file:///e%3A/Handwritten-M2L/venv/lib/site-packages/tensorflow/python/eager/function.py?line=506'>507</a>\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    <a href='file:///e%3A/Handwritten-M2L/venv/lib/site-packages/tensorflow/python/eager/function.py?line=507'>508</a>\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///e%3A/Handwritten-M2L/venv/lib/site-packages/tensorflow/python/eager/function.py?line=510'>511</a>\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    <a href='file:///e%3A/Handwritten-M2L/venv/lib/site-packages/tensorflow/python/eager/function.py?line=511'>512</a>\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32me:\\Handwritten-M2L\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     <a href='file:///e%3A/Handwritten-M2L/venv/lib/site-packages/tensorflow/python/eager/execute.py?line=51'>52</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     <a href='file:///e%3A/Handwritten-M2L/venv/lib/site-packages/tensorflow/python/eager/execute.py?line=52'>53</a>\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> <a href='file:///e%3A/Handwritten-M2L/venv/lib/site-packages/tensorflow/python/eager/execute.py?line=53'>54</a>\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     <a href='file:///e%3A/Handwritten-M2L/venv/lib/site-packages/tensorflow/python/eager/execute.py?line=54'>55</a>\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     <a href='file:///e%3A/Handwritten-M2L/venv/lib/site-packages/tensorflow/python/eager/execute.py?line=55'>56</a>\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     <a href='file:///e%3A/Handwritten-M2L/venv/lib/site-packages/tensorflow/python/eager/execute.py?line=56'>57</a>\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=5, class_weight=labelWeights, use_multiprocessing=True, workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: nnModel2\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"nnModel2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "print(np.argmax(model.predict(symbols[0].reshape(1,45,45))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "dispImages([symbols[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "392167575"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "193663"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_nums.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"nnModel2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "print(np.argmax(model.predict(symbols[0].reshape(1,45,45))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dispImages([symbols[6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5465/5465 [==============================] - 120s 22ms/step - loss: 1.5544 - accuracy: 0.5430\n",
      "Epoch 2/10\n",
      "5465/5465 [==============================] - 120s 22ms/step - loss: 0.8703 - accuracy: 0.7398\n",
      "Epoch 3/10\n",
      "5465/5465 [==============================] - 134s 25ms/step - loss: 0.6568 - accuracy: 0.8006\n",
      "Epoch 4/10\n",
      "5465/5465 [==============================] - 147s 27ms/step - loss: 0.5453 - accuracy: 0.8328\n",
      "Epoch 5/10\n",
      "5465/5465 [==============================] - 142s 26ms/step - loss: 0.4646 - accuracy: 0.8571\n",
      "Epoch 6/10\n",
      "5465/5465 [==============================] - 133s 24ms/step - loss: 0.4109 - accuracy: 0.8715\n",
      "Epoch 7/10\n",
      "5465/5465 [==============================] - 147s 27ms/step - loss: 0.3661 - accuracy: 0.8848\n",
      "Epoch 8/10\n",
      "5465/5465 [==============================] - 149s 27ms/step - loss: 0.3264 - accuracy: 0.8965\n",
      "Epoch 9/10\n",
      "5465/5465 [==============================] - 152s 28ms/step - loss: 0.2941 - accuracy: 0.9051\n",
      "Epoch 10/10\n",
      "5465/5465 [==============================] - 148s 27ms/step - loss: 0.2722 - accuracy: 0.9119\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x276b7a42eb0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(1000, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(1000, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(1000, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(32, activation=tf.nn.softmax))\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: nnModel4\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"nnModel4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "print(np.argmax(model.predict(symbols[6].reshape(1,45,45))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dispImages([symbols[6]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "============================================================================================================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "images=[]\n",
    "images_path = glob.glob(\"mathSymbolsDataset\\1\\*.jpg\")\n",
    "for img_path in images_path:\n",
    "    image = cv2.imread(img_path)\n",
    "    images.append(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(images_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "augumentation = keras.Sequential([\n",
    "    layers.experimental.preprocessing.RandomRotation(10),\n",
    "    layers.experimental.preprocessing.RandomTranslation(width_factor=(0.1,0.1), height_factor=(0.1,0.1))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2769b30fd60>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAB4CAYAAAAJ4bKfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAcwUlEQVR4nO3de3SU1fno8e+T20zukBBCEi4hgIAgJRLAYBSrUiEFvBRbrFJPW+FUbVettudo/Xlq21V7WauivbFqq2K1RbH9FaGAgIAoomgCAnI3QIBwyRWSkMtMZvb5Yyb5BQgkTGbyzkyez1qzMrPn5d3PhuHJnv3uvV8xxqCUUiq8RFgdgFJKKf/T5K6UUmFIk7tSSoUhTe5KKRWGNLkrpVQY0uSulFJhKGDJXUSmi8h+EflcRB4PVD1KKaUuJoGY5y4ikcABYBpwHPgEuMcYs8fvlSmllLpIoHruk4DPjTGHjDEO4HXg9gDVpZRS6gJRATpvFnCs3evjwORLHdyvXz+TnZ192RPW1tZSV1dHeno6UVH+Cfv48eMMHDjwvLJz587hcDjo27evX+o4e/YsdXV1ZGZmEhHR/d+lxhhOnjxJdHQ0MTExOJ1OUlJSEBFExA8RK9W7uFwuIiMjrQ7DJ8XFxZXGmLSO3gtUcu+UiCwAFgAMHjyYoqKiTv/M3/72N8aOHcu1117b7fqNMSxevJhvfvOb55Vv27aNsrIyZs2a1e06AB599FEeffTRi36J+ENZWRlVVVUsWbIEu93OmDFjSEpKYurUqdhsNr/Xp1Q4KSoqol+/fmRlZREdHW11OD4RkdJLvReo5F4GDGr3eqC3rI0x5gXgBYC8vLxOB/5PnDjBG2+8waRJk/wSoIiQm5vL5s2bKSgoaCvfuHEj8+fP90sdADabjUWLFvH000/7/QOUlZVFVlYW48aNwxjDp59+isPhYPny5Xz00UdMmDCB6upqTp8+zYABA0hPT2fOnDl+jUGpUJWSksLgwYP98o06GAUquX8CjBCRoXiS+lzg676erLa2lscee4yCggJGjhzprxhpbGyktrYWY0zbkMa5c+dITEz0Wx3/9V//xSOPPMLSpUu59957/XbeC7X+sgKYNGnSeUn8gw8+wGazMWHChIDVr1Swa2xsxG63t/1fz8nJsTiiwArIbBkAESkEngMigZeMMb+41LF5eXnmcsMyjY2N1NXVkZKS4rfxdgC3283rr7/O3LlziYiIYPfu3djtdoYNG+a3OgCcTiclJSUMGDCAPn36+PXcSqmuKSkpYciQIX7NIVYTkWJjTF5H7wXs+4gxZpUx5ipjzLDLJfZOzoExhk2bNpGUlOT3f5SIiAi2b9+O2+0GYN++fZw6dcqvdQBER0dTVVVFaWlpW5uUUj1r2LBhYZXYOxPUg0379u3j4MGDTJ8+HbvdHpA67r77bt5++23cbjdDhgzhC1/4QkDqmTJlCsOHD2fFihUBOb9SSrUXlMm9ubmZp59+mpSUFK666qqA1pWcnMzRo0dxuVwsXbqUhISEgNQjIsTHxzN79mzefPNN3n777YDUo5RSYOFUyEupqalh7969PPXUUz12FXvlypWUlZVRXV3dI/XNmTOHlpYW1q1bx7Rp03qkTqVUz6qpqSE5Odmy2ThB13NPTk5m3LhxvPzyy9TU1PRInbfccgs/+MEPeuxi58aNGykuLuaGG27okfqUUj0vOTnZ0oWFQddzj4iIIC4ujltuuYX6+npaWlro379/wOqLjo6mf//+9OnTh3HjxgWsnla7du1i7NixREVFBew6glLKelbPnw+6njt4/lKGDh1KQkIC0dHR7Nixg5aWloDUtWLFCgoLC4mKimLw4MHs2rUrIPVUVFRQUVFBfHw8/fv3JzU1NSD1KKUUBGlyb5WSkkLfvn1xu924XK6A1FFfX09ycjIADQ0N1NfXB6SesrIyKisrycnJ0T1glLKAw+HoVdOQgzq5t8rJyeFnP/sZK1eu9Ot5Dx8+zOzZs9u+Pt12222UlJT49QPgdrtZvXo1x48fZ9SoUX4775Vobm6mqampV32wlbpQdXV125qW3iAkkntCQgLTpk3jz3/+M1VVVX47b2lpKYcOHWrrSUdGRrJ9+3a/nR88wz5//OMfufbaawPeY29dILV161ZefvllPv74Y1atWsUzzzzDww8/zPvvvx/Q+pUKZgMGDDhv98dwX1AYdBdUOxIZGcmNN97I1q1bKS0t9ct4tTGG5OTktv1YWhUWFvLee+8xderUbtcBsGnTJhYtWkRmZqZfznchh8PBli1biI+PZ9OmTTidTu677z7sdjtpaWmkpKQwadIknE5nr+q1KNWZlStXMmPGjJDd7rczIZHcAaqqqkhKSvLrDJPXXnuN3/72t+eVxcfHc/ToUb/VkZ6eztmzZ8nIyPDL0me3201JSQk2m421a9dSUlJCfn4+0dHRfO9732vb6nfQoEGdnEmp3m3mzJlWhxBQQZPc2+/MaIyhrq4Om81GfX09breb73znO8ybN4/Ro0f7pb5PPvmEvLyL99vJzs5m8+bNtLS0+CUZP/jggzz44IPMnDmTGTNm4Ha7SUhIoLm5mfj4+EtOl3K5XJw5c4adO3dSVlbGzTffzKJFi7jxxhs5efIkKSkpfOtb3wLQG3UopS4SFMm9vLyc559/nrvvvpusrCwcDgerVq1i9OjRvPXWW8yaNYvFixcTHx/vtyRWUlLS4dYGAwYMoKqqym9DGElJSbz44ou43W5eeeUV6urqmDVrFrt372bChAmkpKSwadMmDh8+TEREBN/4xjdYs2YN2dnZrF27lrFjx3LXXXdhs9n40Y9+hM1m0xtxKKU6FbAtf6/ENddcY9566y0yMzN7ZGGPy+Xik08+ITc3t8NEeezYMZqamhgxYkRA42hqaiIyMpKqqioaGhoAz3BKeXk5ycnJAdvnRikVHizZ8vdK2Gw2cnJyemzFptPpZNmyZZfsAbe0tPh92mVH7HY70dHRDBgwgJycHHJycoiOjiYrK0sTu1KqW4Iiufe0pqYm7r777ku+n5aWRr9+/WhsbOzBqFRv0dTUxPLly60OQ4W5XpncX3755cvuV5OQkEBpaWnAVquq3s3pdOp1kyBnjOHdd9+1Ooxu6XXJvbGxEafTSXx8/GWPGz16NLt37+6hqFRv8ve//51rr73W6jBUJ/r162d1CN3S65J7bW0tLS0tpKSkXPa4W2+9lfXr1/dQVKo3mThxol5TCXIiwtixY60Oo1u6ldxF5IiI7BKRT0WkyFuWIiLrROSg92df/4TqH3V1dYwfP77T4+Li4pg5cybNzc2BD0r1KqWlpTidTqvD8ElTUxObN2+2Oowec/r0aYqLi60Owyf+6Ll/0Rgzvt10nMeB9caYEcB67+ugsXTp0i5tLRAVFcXevXs5cuRI4INSvcrtt99OYmKi1WH4pLGxkXfeecfqMHpM//79Q3YILRDDMrcDr3ifvwLcEYA6fPbII48QFxfXpWPHjh3b6fCNUleqsbExZDesstvtxMbG9tgtKa0Wyqu/u5vcDbBWRIpFZIG3LN0Yc9L7/BSQ3s06/OpPf/pTl6c47ty506+7UCoFntlYVt+lx1exsbEkJiZSWVlpdSiqE93dfqDAGFMmIv2BdSKyr/2bxhgjIh12Uby/DBYADB48uJthdF1CQkKXfxOnpKRw7ty5AEekVGi58847qa2ttToM1YludR+MMWXen+XAv4FJwGkRyQDw/iy/xJ99wRiTZ4zJS0tL604YXXb06FGuuuoqYmNju3T87NmzWbt2bYCjUiq0pKWl8de//tXqMCzjdDo5duxY0G+h7XNyF5F4EUlsfQ58CfgMWA7c7z3sfuCt7gbpL3a7/Yr3gp88eXKAolEqNEVERDB+/PheOzRTX1/P8uXLKS/vsN8aNHzeOExEcvD01sEzvPMPY8wvRCQVWAoMBkqBrxpjLnv1JS8vzxQVFfkUh1Kq561YsYK+fftSUFBgdSiWMMbgdrtxOBxERERYtuL4chuH+Tzmbow5BHyhg/Iq4BZfz6uUCn6pqamcOHHivPsw9CYiQmRkJNHR0UHb/tC8ZK+UstSUKVP48MMPrQ7DclFRUUF7mz5N7kopn9xxxx26gjuIaXJXSvkkLi5Oty4OYprcleohu3btCtmVqR1JTk4mMTExrNoUTjS5K9VDkpOTrQ7Br4YPH86ePXtCdhO0cKfJXakeMnjw4KCdWeGrpqYm6urq/H7exsZGjhw5EpBz9xbd3X5AKdWL3XfffSxdupQHH3ywW+dxuVw0NDRw6NAhVq9ejTEGm83Grbfeyrhx4/wUbe+iyV0p5TMRoaGhwaf57sYY9u7dywcffEBubi7Lli2joKCABx54gKioKPr06ROYoHsJTe5KKZ9lZGTgcDioqqq64tvS1dfX43K5mDJlCtnZ2eTldbjQUvlIk7tSymfR0dFdSuonTpygurq67R7G+fn5JCYmcs011/RAlL2TJnelVLfMmzeP1atXc+edd7aVnTt3jnPnzrFkyRKampq4+uqrqaioYM6cOSQmJobdheVgpMldKdUtLS0tbNmyhcLCQlasWEFlZSUjR45ky5Yt3HPPPaSnpxMbGxuyNygJVZrcleoBTU1NREREEBMTExabbRljaGho4N1336WiooKPPvqIV199lVmzZuF0OsnMzOSLX/yi1WH2aprcleoBERERbT3X/fv3M3ToUMu2ifVVY2Mj27dvp3///rz55ptEREQwffp0UlNTeeWVV8jJybE6RNWOJnelAswYc97WsOvWraOwsJBhw4ZZHNnluVwujh07xoEDB1i3bh3XXXcdxhji4uJ44oknzjv2/fffJyYmhoEDB1oUrbqQJncV1hwOBy0tLcTGxgbNUMi8efNYuHAhP/3pT60O5TzNzc00NzdTXl7OP/7xD26++WZ27NhBdnY2zzzzTNu3j47+Hg8cOECfPn00uQcRTe4qbBhj2jaxWrduHSdPniQ7O5uysjLuuecey5L7hfXa7XYmTJhg+dh769/Xnj17+OCDD6itraW2tpb58+fz0EMPkZCQ0OU7LY0aNYr9+/fr1MYgosldhYXm5mY2bNjA5s2byczMZPbs2WRnZzN8+PCgu5mC3W6nqamJkpIShg8f3uP1b9++nYqKCmw2G8uXL+drX/saN9xwA2lpafh6s/r8/HyefPJJ5syZ4+dola80uauQZYzh4MGDDBgwALvdTmxsLE899RR2u93q0DoVERFBU1OTJXW33h4uPz+fqVOn+uWcIsKCBQv8ci7lH50mdxF5CZgJlBtjxnrLUoA3gGzgCJ6bYNeI5zvm80Ah0AD8L2PMtsCErnorl8uFw+HAbrezZ88e4uLiSEpK4qabbrI6tC77yle+QnV1NS0tLbhcrh6dOROIjbhEhKFDh/r9vMp3XVlVsBiYfkHZ48B6Y8wIYL33NcAMYIT3sQBY5J8wVW9mjKG5uRmn08kbb7zB888/z759+xAR7rjjjqC+iGeMweVyXVQuIqSmpnL27FmOHj1qQWT+V1tbq1v0BpFOe+7GmPdEJPuC4tuBm7zPXwHeBf6vt/xvxnNV6yMR6SMiGcaYk36LWAVc+zvrWH3BDzzj6b/+9a8ZMmQIM2bMoKWlhczMTMviuhLNzc1s376d/Pz8Dt9PTU0lNTW1h6MKjKKiIkREFy8FCV/H3NPbJexTQLr3eRZwrN1xx71lFyV3EVmAp3fP4MGDfQxDBcKGDRt4/fXXefjhhxk/fnyP1+92u9m2bRtlZWV89tlnPP744/zkJz/p8Tj8wW63XzKxX6ihoYGoqChiYmICHFVgxMfHc+zYMctnASmPbm/24O2lX/FNFI0xLxhj8owxeb5eoVf+9e9//5vHH3+ctLQ0fvWrX1mS2F0uF2fPnqWyspLk5GR+/OMfB91sl0DZunUrx48ftzoMn02ePJlDhw5ZHYby8rXnfrp1uEVEMoByb3kZMKjdcQO9ZSoINTc309TURGVlJa+99hq33XYbP//5z4mKirKk51VRUcGBAwdITExk+vQLL/OEpsbGRj766KMuDVVMnTo15Hu8jz32mNUhKC9fk/ty4H7gV96fb7Ur/66IvA5MBs7qeHtwaF2wYoxh1apV1NTUkJGRwXvvvccDDzzAd7/7XeLj44mOjrYktn/961+MGTOGCRMmhMRUxq6Kiopi0KBBnR8IYbFrYm/5lhUKpP3Fsw4PEFmC5+JpP+A08BNgGbAUGAyU4pkKWe2dCvkHPLNrGoBvGmOKOgsiLy/PFBV1epjy0fbt2zl58iQVFRXs2rWL6dOnM2TIEIYNGxYUCcUYw5EjR0hJSSEpKSnke69K9RQRKTbGdHgLq06Te0/Q5O5/DQ0NLFy4kPj4eG699VYqKyuZMmVKyF6sU0pd7HLJXVeohgljDFVVVdTX1/POO+/w5S9/mR/+8IfExMRoTziM7N69m6ysLL15tOqUJvcwsHXrVsrLy6murqahoYH7778/qHZBVP4zcuTIoBhKU8FPk3uIqqmpYePGjYwbN47Ro0czfPjwsFkMEwitw489/QuvubnZr9+eoqL0v6zqGu0ChKBTp06xZs0a3G43SUlJJCUlaWK/jMrKSv7whz9w4sSJHq9bZ48oq2g3IMS0tLTQ1NTEoEGDuP76660OJ+gZY3A4HHz961+35Beg9rSVVcKi537mzBnKy8s7PzBAWueOB9o777xDfX29JvYrlJGRod9sVK8TFsl97969fPjhh5bGsGHDhoCc1xjD4cOHefPNN5kyZQrJycn6Vf8yDh48yOLFi6moqAA8Y+x6YVn1RiGf3I0xlJWV+XwHGX9JT0/n5En/L8Y1xrB161YmTpyoM2AuY8eOHezbt4/09HQKCgpITEy0OiSlLBXyyR3gww8/ZMqUKZbVLyLk5+ezZcsWv5730KFDvPjii8ydO5fs7GxN7B04ffo0f/nLX4iJiSE6OpqkpCSGDx8eFFsYHD9+nP/85z9Wh6F6qZC+2mOMoa6ujrlz51odCiNGjCA1NZXS0lKGDBnil3MuXryYG264wS/nCie1tbV8/PHHxMbGMmXKFB544AHA2r3nO9KnTx+OHj1KY2MjsbGxVoejepmQ7rk7nU5+85vfMHHiRKtDIT09nZiYGFauXOm3c9rtdnJycnA6nX4754WMMQE9vz8YY2hqasLhcFBeXs62bdu4/vrrmTx5ctuYerAldoCEhARuv/12SzZjUyqke+7nzp1j9OjRVofRJiEhAYfDgcPh8MseLvPnz+f73/8+brebefPmUVhY6Jck1rqg59NPP+Wzzz6juLiYhQsXBk2CbL/f0fvvv8/hw4c5cOAAOTk53HvvvSG1NW5WVpbVIaheKqST+7Jly5g2bZrVYbRJT0+nrq6O+vp6UlJSun2+tLQ0nnvuObZs2UJKSgrr169nzJgxZGRk+HQ+YwxFRUU8++yzHDhwgBMnTnDq1CmmTp3Knj17GDNmTLdj7i6n08mxY8dYsmQJSUlJFBYWkpiYyL333qtzxpW6AiH7v8XlclFTU2N1GBdJSkrizJkzfknuAP379+eOO+4APFv3OhwOGhsbiYqKYs2aNUydOpXFixeTm5tLnz59GDt2LC6XiyNHjhAbG8uAAQM4efIkNTU1/PKXv2TNmjUkJSXRt29ffve73xETE8PUqVODZiMqt9uN0+nkySeftDoUpUJayCb31k2ygu1r77x58/j9738fkHt+5ubmAv8zbNE6TPPQQw+dN0zR0tLChg0byMzM5Etf+hLFxcW89NJLrFmzhmnTpvHqq68SHx9PZGRk0A1v2Gw2Ro4caXUYSoW8kE3uNpuNiRMnBl1yio2NJTc3F7fbHbDd+1rb3PrzwkVNNpuN+fPnt72eNWsWBQUFbNiwgRkzZhAfHx+QuJRSwSNkZ8u89NJLQbkEPzY2ltraWkpLS60Opc2ZM2d47rnnuO222zSxK9VLhGxyt9lsQddrbxUTE8PZs2etDqONiDB9+nRdtalULxKSyb2srIxRo0YRFxdndSgduuuuu1i9erXVYbSJiooKmgumSqme0WlyF5GXRKRcRD5rV/a0iJSJyKfeR2G7954Qkc9FZL+I3BaIoKurqzl06FDQ9tzBMxTicrmsDgPwJPekpCSrw1BK9aCu9NwXA9M7KF9ojBnvfawCEJGrgbnAGO+f+ZOI+H0Lw3/+85989atf9fdp/SYyMpIJEybw8ccfWx0K4BkmsnpjNaVUz+o0uRtj3gOqu3i+24HXjTHNxpjDwOfApG7E16H4+PigvjAoImRmZgbNsNGyZcvatsBVSvUO3Rlz/66I7PQO2/T1lmUBx9odc9xbdhERWSAiRSJSdCWJZ+fOneTm5gb1kAxAQUEBxcXF5y2lt0p+fj7r16+3OgylVA/yNbkvAoYB44GTwG+v9ATGmBeMMXnGmLwrGTKorKzE4XAEfXIH2L17t9UhABAXF8eoUaOsDkMp1YN8WsRkjDnd+lxE/gK0blpdBgxqd+hAb9llFRcX14vIfl9iCXbPPvssQD+g0uJQAknbF9q0faHrkvuL+5TcRSTDGNN626E7gdaZNMuBf4jIs0AmMALoylXF/caYPF9iCQUiUqTtC13avtAW7u27lE6Tu4gsAW4C+onIceAnwE0iMh4wwBHgfwMYY3aLyFJgD9ACPGyMCY75gEop1Yt0mtyNMfd0UPziZY7/BfCL7gSllFKqe4JlheoLVgcQYNq+0KbtC23h3r4OSTBM1VNKKeVfwdJzV0op5UeWJ3cRme7dh+ZzEXnc6nh8cYn9d1JEZJ2IHPT+7OstFxH5nbe9O0XkWusi7xoRGSQiG0Vkj4jsFpHve8vDoo0iYheRj0Vkh7d9P/WWDxWRrd52vCEiMd5ym/f15973sy1tQBeISKSIbBeR/3hfh1PbjojILu8+V0XesrD4bHaHpcndu+/MH4EZwNXAPd79aULNYi7ef+dxYL0xZgSw3vsaPG0d4X0swLMgLNi1AI8ZY64GrgMe9v47hUsbm4GbjTFfwLMwb7qIXAf8Gs8eSsOBGuDb3uO/DdR4yxd6jwt23wf2tnsdTm0D+KJ3n6vWKY/h8tn0nTHGsgeQD6xp9/oJ4AkrY+pGW7KBz9q93g9keJ9n4JnLD/Bn4J6OjguVB/AWMC0c2wjEAduAyXgWvkR5y9s+q8AaIN/7PMp7nFgd+2XaNBBPgrsZz4JDCZe2eeM8AvS7oCzsPptX+rB6WKbLe9GEoHTzPwu9TgHp3uch3Wbv1/RcYCth1EbvsMWnQDmwDigBzhhjWryHtG9DW/u8758FUns04CvzHPB/ALf3dSrh0zbwrLdZKyLFIrLAWxY2n01fhew9VEOJMcaISMhPSxKRBOBfwCPGmNr2+/uEehuNZ7HdeBHpA/wbCIvNeERkJlBujCkWkZssDidQCowxZSLSH1gnIvvavxnqn01fWd1z92kvmhBxWkQywLNdA54eIYRom0UkGk9i/7sx5r+9xWHVRgBjzBlgI56hij4i0toBat+GtvZ5308Gqno20i67HpgtIkeA1/EMzTxPeLQNAGNMmfdnOZ5fzJMIw8/mlbI6uX8CjPBeuY/Bc6OP5RbH5C/Lgfu9z+/HM07dWv4N71X764Cz7b4+BiXxdNFfBPYaY55t91ZYtFFE0rw9dkQkFs/1hL14kvwc72EXtq+13XOADcY7gBtsjDFPGGMGGmOy8fz/2mCMuZcwaBuAiMSLSGLrc+BLePa6CovPZrdYPegPFAIH8IxxPml1PD62YQmerY+deMbwvo1nnHI9cBB4B0jxHit4ZgiVALuAPKvj70L7CvCMa+4EPvU+CsOljcA4YLu3fZ8B/89bnoNn47vPgTcBm7fc7n39uff9HKvb0MV23gT8J5za5m3HDu9jd2sOCZfPZnceukJVKaXCkNXDMkoppQJAk7tSSoUhTe5KKRWGNLkrpVQY0uSulFJhSJO7UkqFIU3uSikVhjS5K6VUGPr/Ewo/cvgXxF4AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(augumentation(images[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2769b08ff40>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictt={}\n",
    "for key, value in dic.items():\n",
    "    path = os.path.join('mathSymbolsDataset/', key)\n",
    "    dictt.update(len(os.listdir(path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[47,\n",
       " 258,\n",
       " 477,\n",
       " 556,\n",
       " 558,\n",
       " 693,\n",
       " 973,\n",
       " 1536,\n",
       " 2332,\n",
       " 2689,\n",
       " 2742,\n",
       " 2796,\n",
       " 2909,\n",
       " 3068,\n",
       " 3118,\n",
       " 3251,\n",
       " 3545,\n",
       " 3737,\n",
       " 5140,\n",
       " 5870,\n",
       " 6914,\n",
       " 7396,\n",
       " 9340,\n",
       " 10909,\n",
       " 13104,\n",
       " 14294,\n",
       " 14355,\n",
       " 25112,\n",
       " 26141,\n",
       " 26520,\n",
       " 26594,\n",
       " 33997]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "braket_upsampled = resample(,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=len(), # match number in majority class\n",
    "                          random_state=27) # reproducible results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1912/1912 [==============================] - 99s 51ms/step - loss: 2.9547 - accuracy: 0.1284\n",
      "Epoch 2/5\n",
      "1912/1912 [==============================] - 91s 47ms/step - loss: 2.3326 - accuracy: 0.2792\n",
      "Epoch 3/5\n",
      "1912/1912 [==============================] - 91s 48ms/step - loss: 2.1447 - accuracy: 0.3309\n",
      "Epoch 4/5\n",
      "1912/1912 [==============================] - 98s 51ms/step - loss: 2.0529 - accuracy: 0.3606\n",
      "Epoch 5/5\n",
      "1912/1912 [==============================] - 105s 55ms/step - loss: 1.9897 - accuracy: 0.3784\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1908f7f7940>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(2025, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(2025, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(32, activation=tf.nn.softmax))\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "model.fit(x_train, y_train_nums, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "print(np.argmax(model.predict(symbols[5].reshape(1,45,45))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'classes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Farah\\Documents\\Handwritten-M2L\\farah.ipynb Cell 94'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Farah/Documents/Handwritten-M2L/farah.ipynb#ch0000094?line=0'>1</a>\u001b[0m x_train\u001b[39m.\u001b[39;49mclasses\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'classes'"
     ]
    }
   ],
   "source": [
    "x_train.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_val = lst[-1]      \n",
    "class_weights = {class_id : max_val/num_images for class_id, num_images in counter.items()}                     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "85af8a8472993ce2b507c889893cea6dd7c56561ee4a48c34647ae35980712cd"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
