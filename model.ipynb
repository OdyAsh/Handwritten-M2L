{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting symbols from image using `OpenCV`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import cv2\n",
    "import numpy as np\n",
    "from functools import cmp_to_key\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractSymbols(imgOrig, showSteps = False):\n",
    "    debugImgSteps = []\n",
    "    imgGray = cv2.cvtColor(imgOrig,cv2.COLOR_BGR2GRAY)\n",
    "    imgFiltered = cv2.medianBlur(imgGray, 5)\n",
    "    debugImgSteps.append(imgFiltered)\n",
    "    \n",
    "    imgCanny = cv2.Canny(imgFiltered, 50,180)\n",
    "    debugImgSteps.append(imgCanny)\n",
    "\n",
    "    kernel = np.ones((5,5), np.uint8)\n",
    "    imgDilated = cv2.dilate(imgCanny, kernel, iterations=5)\n",
    "    debugImgSteps.append(imgDilated)\n",
    "\n",
    "    contours, _= cv2.findContours(imgDilated, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "    boundingBoxes = []\n",
    "    for contour in contours:\n",
    "        x,y,w,h = cv2.boundingRect(contour)\n",
    "        boundingBoxes.append((x,y,w,h))\n",
    "\n",
    "    global rowsG\n",
    "    rowsG, _, _ = imgOrig.shape\n",
    "    key_leftRightTopBottom = cmp_to_key(leftRightTopBottom)\n",
    "    boundingBoxes = sorted(boundingBoxes, key=key_leftRightTopBottom)\n",
    "\n",
    "    symbols = []\n",
    "    for (i, box) in enumerate(boundingBoxes):\n",
    "        x,y,w,h = box\n",
    "        mathSymbol = imgOrig[y:y+h, x:x+w]\n",
    "        mathSymbol = cv2.cvtColor(mathSymbol, cv2.COLOR_BGR2GRAY) #converting to Gray as tensorflow deals with grayscale or RGB, not BGR\n",
    "        mathSymbol = cv2.resize(mathSymbol, (45,45), interpolation=cv2.INTER_AREA) #to have the same size as trained images in the dataset\n",
    "        debugImgSteps.append(mathSymbol)\n",
    "        mathSymbolF = mathSymbol.astype('float32') #optional: tensorflows deals with float32, not uint8\n",
    "        symbols.append(mathSymbolF)\n",
    "\n",
    "    if showSteps:\n",
    "        dispImages(debugImgSteps)\n",
    "\n",
    "    return symbols\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leftRightTopBottom(tup1, tup2):\n",
    "    x1, y1, _, _ = tup1\n",
    "    x2, y2, _, _ = tup2\n",
    "    rows = rowsG\n",
    "    yRegion1, yRegion2 = -1, -1\n",
    "\n",
    "    for i in range(8):\n",
    "        if y1 < rows/8 + rows*(i/8):\n",
    "            yRegion1 = i\n",
    "            break\n",
    "    else:\n",
    "        if yRegion1 == -1:\n",
    "            yRegion1 = 8\n",
    "\n",
    "    for i in range(8):\n",
    "        if y2 < rows/8 + rows*(i/8):\n",
    "            yRegion2 = i\n",
    "            break\n",
    "    else:\n",
    "        if yRegion2 == -1:\n",
    "            yRegion2 = 8\n",
    "    \n",
    "    if yRegion1 < yRegion2:\n",
    "        return -1\n",
    "    elif yRegion2 < yRegion1:\n",
    "        return 1\n",
    "    elif x1 <= x2:\n",
    "        return -1\n",
    "    else:\n",
    "        return 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dispImages(imgs):\n",
    "    for img in imgs:\n",
    "        cv2.imshow('Image', img)\n",
    "        cv2.waitKey(0)\n",
    "    else:\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('tests/testMath5.png')\n",
    "symbols = extractSymbols(img, showSteps=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating dictionary that maps folder names to latex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example of folder names in \"mathSymbolsDataset\": <br>\n",
    "<img src=\"guideImages/datasetFolders.png\" width=400 height=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using `r` to make the string `raw` to avoid confusing strings like `\\n` with python's new line <br>\n",
    "however, the values will now have two backslashes (e.g. `\\\\n`), thus, we will later need to replace each `\\\\` with `\\`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {\n",
    "    \"-\": r\"-\",\n",
    "    \"!\": r\"!\",\n",
    "    \"(\": r\"(\",\n",
    "    \")\": r\")\",\n",
    "    \",\": r\",\",\n",
    "    \"[\": r\"[\",\n",
    "    \"]\": r\"]\",\n",
    "    \"{\": r\"\\{\",\n",
    "    \"}\": r\"\\}\",\n",
    "    \"+\": r\"+\",\n",
    "    \"=\": r\"=\",\n",
    "    \"0\": r\"0\",\n",
    "    \"1\": r\"1\",\n",
    "    \"2\": r\"2\",\n",
    "    \"3\": r\"3\",\n",
    "    \"4\": r\"4\",\n",
    "    \"5\": r\"5\",\n",
    "    \"6\": r\"6\",\n",
    "    \"7\": r\"7\",\n",
    "    \"8\": r\"8\",\n",
    "    \"9\": r\"9\",\n",
    "    \"A\": r\"\\A\",\n",
    "    \"alpha\": r\"\\alpha\",\n",
    "    \"b\": r\"b\",\n",
    "    \"beta\": r\"\\beta\",\n",
    "    \"C\": r\"\\C\",\n",
    "    \"cos\": r\"\\cos\",\n",
    "    \"d\": r\"d\",\n",
    "    \"Delta\": r\"\\Delta\",\n",
    "    \"div\": r\"\\div\",\n",
    "    \"e\": r\"exp()\",\n",
    "    \"exists\": r\"\\exists\",\n",
    "    \"f\": r\"f\",\n",
    "    \"forall\": r\"\\forall\",\n",
    "    \"forward_slash\": r\"/\",\n",
    "    \"G\": r\"\\G\",\n",
    "    \"gamma\": r\"\\gamma\",\n",
    "    \"geq\": r\"\\geq\",\n",
    "    \"gt\": r\">\",\n",
    "    \"H\": r\"\\H\",\n",
    "    \"i\": r\"i\",\n",
    "    \"in\": r\"\\in\",\n",
    "    \"infty\": r\"\\infty\",\n",
    "    \"int\": r\"\\int\",\n",
    "    \"j\": r\"j\",\n",
    "    \"k\": r\"k\",\n",
    "    \"l\": r\"l\",\n",
    "    \"lambda\": r\"\\lambda\",\n",
    "    \"ldots\": r\"\\ldots\",\n",
    "    \"leq\": r\"\\le\",\n",
    "    \"lim\": r\"\\lim\",\n",
    "    \"log\": r\"\\log\",\n",
    "    \"lt\": r\"<\",\n",
    "    \"M\": r\"\\M\",\n",
    "    \"mu\": r\"\\mu\",\n",
    "    \"N\": r\"\\N\",\n",
    "    \"neq\": r\"\\neq\",\n",
    "    \"o\": r\"\\O\",\n",
    "    \"p\": r\"p\",\n",
    "    \"phi\": r\"\\Phi\",\n",
    "    \"pi\": r\"\\Pi\",\n",
    "    \"pm\": r\"\\pm\",\n",
    "    \"q\": r\"q\",\n",
    "    \"R\": r\"\\R\",\n",
    "    \"rightarrow\": r\"\\rightarrow\",\n",
    "    \"S\": r\"\\S\",\n",
    "    \"sigma\": r\"\\sigma\",\n",
    "    \"sin\": r\"\\sin\",\n",
    "    \"sum\": r\"\\sum\",\n",
    "    \"T\": r\"\\T\",\n",
    "    \"tan\": r\"\\tan\",\n",
    "    \"theta\": r\"\\theta\",\n",
    "    \"times\": r\"\\times\",\n",
    "    \"u\": r\"u\",\n",
    "    \"v\": r\"v\",\n",
    "    \"w\": r\"w\",\n",
    "    \"X\": r\"\\X\",\n",
    "    \"y\": r\"y\",\n",
    "    \"z\": r\"z\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the kaggle [dataset](https://www.kaggle.com/datasets/xainano/handwrittenmathsymbols?resource=download)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "1. create a list of images and another list of labels for each image\n",
    "2. store them in pickle files for easy retrieval when re-running the code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(dataDir):\n",
    "    imgs = []\n",
    "    labels = []\n",
    "    for key, value in dic.items():\n",
    "        path = os.path.join(dataDir, key)\n",
    "        for imgName in os.listdir(path):\n",
    "            try:\n",
    "                img = cv2.imread(os.path.join(path, imgName), cv2.COLOR_BGR2GRAY) \n",
    "                imgs.append(img)\n",
    "                labels.append(value)\n",
    "            except Exception as e:\n",
    "                print(e)    \n",
    "    return (imgs, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell is commented as it takes a long time (10min if image RGB, 1min otherwise) to create the pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imgs, labels = loadData('mathSymbolsDataset/')\n",
    "#with open(\"x_symbols.pickle\", 'wb') as f:\n",
    "#    pickle.dump(imgs, f)\n",
    "#with open(\"y_latex.pickle\", 'wb') as f:\n",
    "#    pickle.dump(labels, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"x_symbols.pickle\", 'rb') as f:\n",
    "    imgs = pickle.load(f)\n",
    "with open(\"y_latex.pickle\", 'rb') as f:\n",
    "    labels = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## converting text labels (latex) to numeric codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!': 0,\n",
       " '(': 1,\n",
       " ')': 2,\n",
       " '+': 3,\n",
       " ',': 4,\n",
       " '-': 5,\n",
       " '/': 6,\n",
       " '0': 7,\n",
       " '1': 8,\n",
       " '2': 9,\n",
       " '3': 10,\n",
       " '4': 11,\n",
       " '5': 12,\n",
       " '6': 13,\n",
       " '7': 14,\n",
       " '8': 15,\n",
       " '9': 16,\n",
       " '<': 17,\n",
       " '=': 18,\n",
       " '>': 19,\n",
       " '[': 20,\n",
       " '\\\\A': 21,\n",
       " '\\\\C': 22,\n",
       " '\\\\Delta': 23,\n",
       " '\\\\G': 24,\n",
       " '\\\\H': 25,\n",
       " '\\\\M': 26,\n",
       " '\\\\N': 27,\n",
       " '\\\\O': 28,\n",
       " '\\\\Phi': 29,\n",
       " '\\\\Pi': 30,\n",
       " '\\\\R': 31,\n",
       " '\\\\S': 32,\n",
       " '\\\\T': 33,\n",
       " '\\\\X': 34,\n",
       " '\\\\alpha': 35,\n",
       " '\\\\beta': 36,\n",
       " '\\\\cos': 37,\n",
       " '\\\\div': 38,\n",
       " '\\\\exists': 39,\n",
       " '\\\\forall': 40,\n",
       " '\\\\gamma': 41,\n",
       " '\\\\geq': 42,\n",
       " '\\\\in': 43,\n",
       " '\\\\infty': 44,\n",
       " '\\\\int': 45,\n",
       " '\\\\lambda': 46,\n",
       " '\\\\ldots': 47,\n",
       " '\\\\le': 48,\n",
       " '\\\\lim': 49,\n",
       " '\\\\log': 50,\n",
       " '\\\\mu': 51,\n",
       " '\\\\neq': 52,\n",
       " '\\\\pm': 53,\n",
       " '\\\\rightarrow': 54,\n",
       " '\\\\sigma': 55,\n",
       " '\\\\sin': 56,\n",
       " '\\\\sum': 57,\n",
       " '\\\\tan': 58,\n",
       " '\\\\theta': 59,\n",
       " '\\\\times': 60,\n",
       " '\\\\{': 61,\n",
       " '\\\\}': 62,\n",
       " ']': 63,\n",
       " 'b': 64,\n",
       " 'd': 65,\n",
       " 'exp()': 66,\n",
       " 'f': 67,\n",
       " 'i': 68,\n",
       " 'j': 69,\n",
       " 'k': 70,\n",
       " 'l': 71,\n",
       " 'p': 72,\n",
       " 'q': 73,\n",
       " 'u': 74,\n",
       " 'v': 75,\n",
       " 'w': 76,\n",
       " 'y': 77,\n",
       " 'z': 78}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latexToNums = {k: v for v, k in enumerate(np.unique(labels))}\n",
    "#this dictionary is to revert the predicted numeric code back to latex: \n",
    "numsToLatex = {v: k for v, k in enumerate(np.unique(labels))}\n",
    "latexToNums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data into train and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `stratify` is used to split the dataset into train and test sets <br> \n",
    "in a way that preserves the same proportions of examples in each class as observed in the original dataset <br>\n",
    "[(source)](https://machinelearningmastery.com/train-test-split-for-evaluating-machine-learning-algorithms/#:~:text=is%20desirable%20to-,split%20the%20dataset%20into,stratified%20train-test%20split.,-We%20can%20achieve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(imgs, labels, test_size=0.33, stratify=labels, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing image pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tf.keras.utils.normalize(x_train, axis=1) #similar to dividing by 255 (but not equivalent in result)\n",
    "x_test = tf.keras.utils.normalize(x_test, axis=1) #Also, don't know why we are using \"axis=1\" specifically, but that's what's normally used with image normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting `y` labels to numeric codes instead of strings\n",
    "Because `keras` models accept numbers not strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[58,\n",
       " 65,\n",
       " 10,\n",
       " 62,\n",
       " 5,\n",
       " 9,\n",
       " 31,\n",
       " 34,\n",
       " 77,\n",
       " 3,\n",
       " 34,\n",
       " 5,\n",
       " 11,\n",
       " 9,\n",
       " 34,\n",
       " 30,\n",
       " 21,\n",
       " 5,\n",
       " 5,\n",
       " 14,\n",
       " 27,\n",
       " 5,\n",
       " 5,\n",
       " 10,\n",
       " 77,\n",
       " 64,\n",
       " 21,\n",
       " 34,\n",
       " 8,\n",
       " 34,\n",
       " 3,\n",
       " 27,\n",
       " 16,\n",
       " 67,\n",
       " 5,\n",
       " 18,\n",
       " 34,\n",
       " 3,\n",
       " 77,\n",
       " 3,\n",
       " 5,\n",
       " 8,\n",
       " 18,\n",
       " 56,\n",
       " 5,\n",
       " 9,\n",
       " 8,\n",
       " 1,\n",
       " 34,\n",
       " 5,\n",
       " 45,\n",
       " 9,\n",
       " 15,\n",
       " 60,\n",
       " 8,\n",
       " 11,\n",
       " 50,\n",
       " 8,\n",
       " 1,\n",
       " 22,\n",
       " 21,\n",
       " 9,\n",
       " 21,\n",
       " 63,\n",
       " 9,\n",
       " 5,\n",
       " 9,\n",
       " 26,\n",
       " 31,\n",
       " 60,\n",
       " 9,\n",
       " 3,\n",
       " 18,\n",
       " 3,\n",
       " 34,\n",
       " 8,\n",
       " 5,\n",
       " 66,\n",
       " 34,\n",
       " 21,\n",
       " 15,\n",
       " 30,\n",
       " 49,\n",
       " 5,\n",
       " 10,\n",
       " 3,\n",
       " 14,\n",
       " 5,\n",
       " 7,\n",
       " 18,\n",
       " 67,\n",
       " 49,\n",
       " 37,\n",
       " 37,\n",
       " 16,\n",
       " 10,\n",
       " 5,\n",
       " 56,\n",
       " 15,\n",
       " 5,\n",
       " 10,\n",
       " 11,\n",
       " 2,\n",
       " 22,\n",
       " 8,\n",
       " 64,\n",
       " 73,\n",
       " 27,\n",
       " 66,\n",
       " 22,\n",
       " 8,\n",
       " 59,\n",
       " 36,\n",
       " 34,\n",
       " 8,\n",
       " 9,\n",
       " 5,\n",
       " 77,\n",
       " 8,\n",
       " 18,\n",
       " 30,\n",
       " 26,\n",
       " 3,\n",
       " 70,\n",
       " 16,\n",
       " 18,\n",
       " 64,\n",
       " 1,\n",
       " 78,\n",
       " 1,\n",
       " 5,\n",
       " 34,\n",
       " 34,\n",
       " 9,\n",
       " 10,\n",
       " 8,\n",
       " 60,\n",
       " 1,\n",
       " 12,\n",
       " 36,\n",
       " 5,\n",
       " 23,\n",
       " 5,\n",
       " 27,\n",
       " 34,\n",
       " 8,\n",
       " 0,\n",
       " 49,\n",
       " 34,\n",
       " 9,\n",
       " 3,\n",
       " 48,\n",
       " 5,\n",
       " 57,\n",
       " 59,\n",
       " 3,\n",
       " 2,\n",
       " 21,\n",
       " 69,\n",
       " 21,\n",
       " 49,\n",
       " 3,\n",
       " 34,\n",
       " 12,\n",
       " 5,\n",
       " 2,\n",
       " 5,\n",
       " 73,\n",
       " 34,\n",
       " 5,\n",
       " 5,\n",
       " 41,\n",
       " 24,\n",
       " 3,\n",
       " 3,\n",
       " 14,\n",
       " 64,\n",
       " 27,\n",
       " 77,\n",
       " 34,\n",
       " 7,\n",
       " 58,\n",
       " 50,\n",
       " 22,\n",
       " 7,\n",
       " 26,\n",
       " 10,\n",
       " 68,\n",
       " 5,\n",
       " 5,\n",
       " 78,\n",
       " 3,\n",
       " 59,\n",
       " 12,\n",
       " 77,\n",
       " 78,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 50,\n",
       " 47,\n",
       " 65,\n",
       " 5,\n",
       " 72,\n",
       " 37,\n",
       " 3,\n",
       " 3,\n",
       " 34,\n",
       " 37,\n",
       " 9,\n",
       " 56,\n",
       " 9,\n",
       " 16,\n",
       " 8,\n",
       " 5,\n",
       " 14,\n",
       " 27,\n",
       " 37,\n",
       " 64,\n",
       " 5,\n",
       " 5,\n",
       " 27,\n",
       " 59,\n",
       " 45,\n",
       " 16,\n",
       " 38,\n",
       " 2,\n",
       " 12,\n",
       " 68,\n",
       " 21,\n",
       " 11,\n",
       " 36,\n",
       " 12,\n",
       " 8,\n",
       " 34,\n",
       " 9,\n",
       " 34,\n",
       " 2,\n",
       " 1,\n",
       " 74,\n",
       " 5,\n",
       " 3,\n",
       " 10,\n",
       " 1,\n",
       " 3,\n",
       " 27,\n",
       " 3,\n",
       " 34,\n",
       " 10,\n",
       " 8,\n",
       " 68,\n",
       " 3,\n",
       " 22,\n",
       " 70,\n",
       " 60,\n",
       " 14,\n",
       " 2,\n",
       " 24,\n",
       " 3,\n",
       " 34,\n",
       " 48,\n",
       " 9,\n",
       " 8,\n",
       " 2,\n",
       " 9,\n",
       " 5,\n",
       " 3,\n",
       " 72,\n",
       " 3,\n",
       " 72,\n",
       " 31,\n",
       " 18,\n",
       " 5,\n",
       " 3,\n",
       " 18,\n",
       " 78,\n",
       " 9,\n",
       " 5,\n",
       " 18,\n",
       " 63,\n",
       " 9,\n",
       " 9,\n",
       " 18,\n",
       " 3,\n",
       " 5,\n",
       " 64,\n",
       " 77,\n",
       " 64,\n",
       " 32,\n",
       " 9,\n",
       " 65,\n",
       " 18,\n",
       " 5,\n",
       " 68,\n",
       " 1,\n",
       " 34,\n",
       " 8,\n",
       " 5,\n",
       " 34,\n",
       " 21,\n",
       " 14,\n",
       " 44,\n",
       " 37,\n",
       " 21,\n",
       " 5,\n",
       " 5,\n",
       " 9,\n",
       " 8,\n",
       " 3,\n",
       " 3,\n",
       " 34,\n",
       " 78,\n",
       " 18,\n",
       " 18,\n",
       " 42,\n",
       " 8,\n",
       " 14,\n",
       " 9,\n",
       " 32,\n",
       " 68,\n",
       " 5,\n",
       " 8,\n",
       " 14,\n",
       " 9,\n",
       " 3,\n",
       " 1,\n",
       " 5,\n",
       " 72,\n",
       " 68,\n",
       " 5,\n",
       " 77,\n",
       " 1,\n",
       " 5,\n",
       " 9,\n",
       " 10,\n",
       " 62,\n",
       " 3,\n",
       " 5,\n",
       " 8,\n",
       " 18,\n",
       " 36,\n",
       " 67,\n",
       " 5,\n",
       " 29,\n",
       " 1,\n",
       " 21,\n",
       " 4,\n",
       " 8,\n",
       " 66,\n",
       " 8,\n",
       " 78,\n",
       " 11,\n",
       " 73,\n",
       " 34,\n",
       " 65,\n",
       " 5,\n",
       " 18,\n",
       " 72,\n",
       " 60,\n",
       " 9,\n",
       " 34,\n",
       " 37,\n",
       " 3,\n",
       " 10,\n",
       " 26,\n",
       " 8,\n",
       " 36,\n",
       " 2,\n",
       " 56,\n",
       " 5,\n",
       " 65,\n",
       " 11,\n",
       " 16,\n",
       " 34,\n",
       " 3,\n",
       " 18,\n",
       " 9,\n",
       " 7,\n",
       " 7,\n",
       " 57,\n",
       " 3,\n",
       " 56,\n",
       " 9,\n",
       " 34,\n",
       " 2,\n",
       " 72,\n",
       " 5,\n",
       " 37,\n",
       " 73,\n",
       " 15,\n",
       " 3,\n",
       " 8,\n",
       " 5,\n",
       " 21,\n",
       " 65,\n",
       " 34,\n",
       " 34,\n",
       " 18,\n",
       " 65,\n",
       " 3,\n",
       " 8,\n",
       " 78,\n",
       " 57,\n",
       " 7,\n",
       " 64,\n",
       " 57,\n",
       " 8,\n",
       " 2,\n",
       " 67,\n",
       " 9,\n",
       " 75,\n",
       " 10,\n",
       " 75,\n",
       " 2,\n",
       " 2,\n",
       " 27,\n",
       " 72,\n",
       " 27,\n",
       " 12,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 34,\n",
       " 18,\n",
       " 71,\n",
       " 8,\n",
       " 34,\n",
       " 56,\n",
       " 18,\n",
       " 7,\n",
       " 65,\n",
       " 8,\n",
       " 70,\n",
       " 12,\n",
       " 9,\n",
       " 7,\n",
       " 15,\n",
       " 33,\n",
       " 59,\n",
       " 21,\n",
       " 63,\n",
       " 9,\n",
       " 21,\n",
       " 34,\n",
       " 44,\n",
       " 78,\n",
       " 42,\n",
       " 11,\n",
       " 3,\n",
       " 5,\n",
       " 34,\n",
       " 9,\n",
       " 11,\n",
       " 21,\n",
       " 8,\n",
       " 34,\n",
       " 9,\n",
       " 5,\n",
       " 7,\n",
       " 5,\n",
       " 34,\n",
       " 27,\n",
       " 9,\n",
       " 9,\n",
       " 1,\n",
       " 1,\n",
       " 77,\n",
       " 1,\n",
       " 48,\n",
       " 8,\n",
       " 64,\n",
       " 8,\n",
       " 5,\n",
       " 59,\n",
       " 9,\n",
       " 34,\n",
       " 3,\n",
       " 67,\n",
       " 34,\n",
       " 2,\n",
       " 21,\n",
       " 3,\n",
       " 34,\n",
       " 1,\n",
       " 21,\n",
       " 77,\n",
       " 8,\n",
       " 27,\n",
       " 49,\n",
       " 27,\n",
       " 26,\n",
       " 32,\n",
       " 2,\n",
       " 2,\n",
       " 10,\n",
       " 2,\n",
       " 2,\n",
       " 9,\n",
       " 34,\n",
       " 27,\n",
       " 77,\n",
       " 5,\n",
       " 56,\n",
       " 1,\n",
       " 7,\n",
       " 18,\n",
       " 5,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 77,\n",
       " 5,\n",
       " 70,\n",
       " 3,\n",
       " 37,\n",
       " 1,\n",
       " 34,\n",
       " 5,\n",
       " 5,\n",
       " 21,\n",
       " 5,\n",
       " 9,\n",
       " 29,\n",
       " 13,\n",
       " 8,\n",
       " 1,\n",
       " 27,\n",
       " 10,\n",
       " 5,\n",
       " 9,\n",
       " 5,\n",
       " 57,\n",
       " 11,\n",
       " 8,\n",
       " 14,\n",
       " 13,\n",
       " 3,\n",
       " 2,\n",
       " 8,\n",
       " 55,\n",
       " 9,\n",
       " 27,\n",
       " 21,\n",
       " 34,\n",
       " 5,\n",
       " 13,\n",
       " 25,\n",
       " 10,\n",
       " 3,\n",
       " 34,\n",
       " 9,\n",
       " 9,\n",
       " 74,\n",
       " 2,\n",
       " 34,\n",
       " 5,\n",
       " 7,\n",
       " 27,\n",
       " 34,\n",
       " 8,\n",
       " 2,\n",
       " 14,\n",
       " 78,\n",
       " 18,\n",
       " 69,\n",
       " 78,\n",
       " 2,\n",
       " 5,\n",
       " 41,\n",
       " 7,\n",
       " 21,\n",
       " 57,\n",
       " 78,\n",
       " 3,\n",
       " 78,\n",
       " 12,\n",
       " 5,\n",
       " 18,\n",
       " 53,\n",
       " 5,\n",
       " 1,\n",
       " 8,\n",
       " 70,\n",
       " 12,\n",
       " 13,\n",
       " 21,\n",
       " 11,\n",
       " 8,\n",
       " 18,\n",
       " 5,\n",
       " 2,\n",
       " 9,\n",
       " 67,\n",
       " 2,\n",
       " 8,\n",
       " 59,\n",
       " 1,\n",
       " 5,\n",
       " 8,\n",
       " 11,\n",
       " 49,\n",
       " 2,\n",
       " 21,\n",
       " 8,\n",
       " 58,\n",
       " 9,\n",
       " 31,\n",
       " 59,\n",
       " 5,\n",
       " 18,\n",
       " 8,\n",
       " 9,\n",
       " 1,\n",
       " 77,\n",
       " 8,\n",
       " 3,\n",
       " 68,\n",
       " 34,\n",
       " 27,\n",
       " 9,\n",
       " 21,\n",
       " 16,\n",
       " 67,\n",
       " 45,\n",
       " 76,\n",
       " 20,\n",
       " 8,\n",
       " 3,\n",
       " 5,\n",
       " 2,\n",
       " 67,\n",
       " 9,\n",
       " 1,\n",
       " 11,\n",
       " 3,\n",
       " 9,\n",
       " 5,\n",
       " 77,\n",
       " 3,\n",
       " 21,\n",
       " 53,\n",
       " 5,\n",
       " 65,\n",
       " 16,\n",
       " 27,\n",
       " 3,\n",
       " 15,\n",
       " 2,\n",
       " 18,\n",
       " 5,\n",
       " 34,\n",
       " 15,\n",
       " 2,\n",
       " 65,\n",
       " 9,\n",
       " 1,\n",
       " 8,\n",
       " 77,\n",
       " 36,\n",
       " 16,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 8,\n",
       " 22,\n",
       " 3,\n",
       " 10,\n",
       " 5,\n",
       " 72,\n",
       " 3,\n",
       " 11,\n",
       " 3,\n",
       " 2,\n",
       " 8,\n",
       " 60,\n",
       " 1,\n",
       " 67,\n",
       " 30,\n",
       " 3,\n",
       " 13,\n",
       " 9,\n",
       " 34,\n",
       " 47,\n",
       " 11,\n",
       " 10,\n",
       " 48,\n",
       " 67,\n",
       " 34,\n",
       " 33,\n",
       " 27,\n",
       " 9,\n",
       " 3,\n",
       " 78,\n",
       " 9,\n",
       " 34,\n",
       " 64,\n",
       " 5,\n",
       " 5,\n",
       " 34,\n",
       " 10,\n",
       " 2,\n",
       " 10,\n",
       " 26,\n",
       " 5,\n",
       " 1,\n",
       " 30,\n",
       " 5,\n",
       " 59,\n",
       " 5,\n",
       " 34,\n",
       " 11,\n",
       " 60,\n",
       " 34,\n",
       " 8,\n",
       " 8,\n",
       " 22,\n",
       " 52,\n",
       " 8,\n",
       " 10,\n",
       " 1,\n",
       " 11,\n",
       " 5,\n",
       " 78,\n",
       " 9,\n",
       " 69,\n",
       " 21,\n",
       " 21,\n",
       " 1,\n",
       " 34,\n",
       " 4,\n",
       " 34,\n",
       " 9,\n",
       " 9,\n",
       " 5,\n",
       " 5,\n",
       " 45,\n",
       " 8,\n",
       " 31,\n",
       " 68,\n",
       " 10,\n",
       " 64,\n",
       " 56,\n",
       " 77,\n",
       " 1,\n",
       " 77,\n",
       " 66,\n",
       " 60,\n",
       " 2,\n",
       " 36,\n",
       " 5,\n",
       " 9,\n",
       " 5,\n",
       " 3,\n",
       " 64,\n",
       " 34,\n",
       " 5,\n",
       " 34,\n",
       " 34,\n",
       " 65,\n",
       " 34,\n",
       " 8,\n",
       " 5,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 5,\n",
       " 21,\n",
       " 9,\n",
       " 9,\n",
       " 21,\n",
       " 24,\n",
       " 8,\n",
       " 69,\n",
       " 42,\n",
       " 3,\n",
       " 34,\n",
       " 75,\n",
       " 2,\n",
       " 16,\n",
       " 8,\n",
       " 0,\n",
       " 64,\n",
       " 68,\n",
       " 8,\n",
       " 5,\n",
       " 7,\n",
       " 10,\n",
       " 50,\n",
       " 3,\n",
       " 68,\n",
       " 5,\n",
       " 26,\n",
       " 8,\n",
       " 34,\n",
       " 72,\n",
       " 3,\n",
       " 13,\n",
       " 5,\n",
       " 21,\n",
       " 14,\n",
       " 8,\n",
       " 18,\n",
       " 64,\n",
       " 38,\n",
       " 34,\n",
       " 8,\n",
       " 34,\n",
       " 25,\n",
       " 34,\n",
       " 34,\n",
       " 9,\n",
       " 65,\n",
       " 34,\n",
       " 8,\n",
       " 8,\n",
       " 16,\n",
       " 21,\n",
       " 3,\n",
       " 9,\n",
       " 21,\n",
       " 21,\n",
       " 73,\n",
       " 8,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 31,\n",
       " 21,\n",
       " 78,\n",
       " 5,\n",
       " 34,\n",
       " 8,\n",
       " 9,\n",
       " 74,\n",
       " 7,\n",
       " 34,\n",
       " 65,\n",
       " 60,\n",
       " 65,\n",
       " 9,\n",
       " 67,\n",
       " 22,\n",
       " 3,\n",
       " 9,\n",
       " 54,\n",
       " 18,\n",
       " 5,\n",
       " 5,\n",
       " 77,\n",
       " 77,\n",
       " 78,\n",
       " 27,\n",
       " 11,\n",
       " 27,\n",
       " 69,\n",
       " 61,\n",
       " 1,\n",
       " 2,\n",
       " 59,\n",
       " 5,\n",
       " 1,\n",
       " 8,\n",
       " 5,\n",
       " 9,\n",
       " 27,\n",
       " 34,\n",
       " 5,\n",
       " 34,\n",
       " 21,\n",
       " 45,\n",
       " 16,\n",
       " 3,\n",
       " 8,\n",
       " 5,\n",
       " 33,\n",
       " 5,\n",
       " 56,\n",
       " 56,\n",
       " 3,\n",
       " 8,\n",
       " 9,\n",
       " 32,\n",
       " 11,\n",
       " 27,\n",
       " 64,\n",
       " 68,\n",
       " 72,\n",
       " 11,\n",
       " 34,\n",
       " 3,\n",
       " 3,\n",
       " 10,\n",
       " 34,\n",
       " 31,\n",
       " 7,\n",
       " 21,\n",
       " 1,\n",
       " 3,\n",
       " 18,\n",
       " 30,\n",
       " 72,\n",
       " 21,\n",
       " 8,\n",
       " 18,\n",
       " 54,\n",
       " 1,\n",
       " 34,\n",
       " 16,\n",
       " 56,\n",
       " 70,\n",
       " 18,\n",
       " 37,\n",
       " 8,\n",
       " 2,\n",
       " 77,\n",
       " 65,\n",
       " 64,\n",
       " 27,\n",
       " 57,\n",
       " 1,\n",
       " 9,\n",
       " 18,\n",
       " 5,\n",
       " 18,\n",
       " 21,\n",
       " 27,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 22,\n",
       " 64,\n",
       " 78,\n",
       " 9,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 20,\n",
       " 10,\n",
       " 9,\n",
       " 34,\n",
       " 9,\n",
       " 60,\n",
       " 5,\n",
       " 5,\n",
       " 22,\n",
       " 64,\n",
       " 32,\n",
       " 9,\n",
       " 31,\n",
       " 34,\n",
       " 31,\n",
       " 33,\n",
       " 45,\n",
       " 2,\n",
       " 68,\n",
       " 21,\n",
       " 67,\n",
       " 27,\n",
       " 27,\n",
       " 9,\n",
       " 12,\n",
       " 12,\n",
       " 5,\n",
       " 44,\n",
       " 5,\n",
       " 54,\n",
       " 37,\n",
       " 12,\n",
       " 9,\n",
       " 37,\n",
       " 18,\n",
       " 70,\n",
       " 11,\n",
       " 11,\n",
       " 5,\n",
       " 12,\n",
       " 10,\n",
       " 34,\n",
       " 1,\n",
       " 5,\n",
       " 77,\n",
       " 1,\n",
       " 72,\n",
       " 34,\n",
       " 78,\n",
       " 1,\n",
       " 67,\n",
       " 25,\n",
       " 8,\n",
       " 68,\n",
       " 21,\n",
       " 37,\n",
       " 52,\n",
       " 72,\n",
       " 37,\n",
       " 34,\n",
       " ...]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_nums = [latexToNums[latex] for latex in y_train]\n",
    "y_test_nums = [latexToNums[latex] for latex in y_test]\n",
    "y_train_nums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making sure all datasets are `ndarray` not `list`\n",
    "Because `keras` models accept `ndarray`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, numpy.ndarray, list, list)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x_train), type(x_test), type(y_train_nums), type(y_test_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_nums = np.array(y_train_nums)\n",
    "y_test_nums = np.array(y_test_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, numpy.ndarray)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_train_nums), type(y_test_nums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Sequential vs Functional models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sequential is a linear stack of layers. In other words, the layer `i` is connected only to layers `i-1` and `i+1`\n",
    "* Functional is more dynamic, as each layer can connect to any other layer in the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the images are small in size, and the problem is relatively simple, we'll use a sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Model layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for easier processing: flatten image (e.g. 45x45 will become 1x2025)\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "# 128 nodes are chosen as they are a power of 2 (2^7) which makes computation easier, and the images are not large (45x45) so 128 nodes should suffice\n",
    "# relu is the default activation function to use\n",
    "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\n",
    "# add another layer because if you have one, then you're getting linear relations only between the image's features, while two layers makes it non-linear\n",
    "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\n",
    "# number of classifications == number of stored latex strings == len(latexToNums) == 79\n",
    "# using softmax as it converts the scores to a normalized probability distribution\n",
    "model.add(tf.keras.layers.Dense(len(latexToNums), activation=tf.nn.softmax))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"compiling\" means passing the settings for actually optimizing/training the model we've defined\n",
    "model.compile(optimizer='adam', # same logic as relu, great default optimizer to start with\n",
    "              loss='sparse_categorical_crossentropy', # A neural network doesn't actually attempt to maximize accuracy. It attempts to minimize loss, this loss function is also a great default\n",
    "              metrics=['accuracy']) # ratio between the number of correct predictions to the total number of predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"A good rule of thumb is to start with a value that is 3 times the number of columns in your data.\" <br>\n",
    "[(source)](https://gretel.ai/gretel-synthetics-faqs/how-many-epochs-should-i-train-my-model-with) <br>\n",
    "Therefore, we start by with 45*3 = 135 epochs (i.e. number of passes of the entire training dataset the machine learning algorithm has completed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/135\n",
      "7651/7651 [==============================] - 18s 2ms/step - loss: 2.1053 - accuracy: 0.4697\n",
      "Epoch 2/135\n",
      "7651/7651 [==============================] - 15s 2ms/step - loss: 1.2277 - accuracy: 0.6756\n",
      "Epoch 3/135\n",
      "7651/7651 [==============================] - 18s 2ms/step - loss: 0.9650 - accuracy: 0.7408\n",
      "Epoch 4/135\n",
      "7651/7651 [==============================] - 17s 2ms/step - loss: 0.8181 - accuracy: 0.7774\n",
      "Epoch 5/135\n",
      "7651/7651 [==============================] - 13s 2ms/step - loss: 0.7263 - accuracy: 0.8003\n",
      "Epoch 6/135\n",
      "7651/7651 [==============================] - 13s 2ms/step - loss: 0.6606 - accuracy: 0.8170\n",
      "Epoch 7/135\n",
      "7651/7651 [==============================] - 12s 2ms/step - loss: 0.6112 - accuracy: 0.8293\n",
      "Epoch 8/135\n",
      "7651/7651 [==============================] - 12s 2ms/step - loss: 0.5715 - accuracy: 0.8394\n",
      "Epoch 9/135\n",
      "7651/7651 [==============================] - 13s 2ms/step - loss: 0.5393 - accuracy: 0.8475\n",
      "Epoch 10/135\n",
      "7651/7651 [==============================] - 14s 2ms/step - loss: 0.5153 - accuracy: 0.8530\n",
      "Epoch 11/135\n",
      "7651/7651 [==============================] - 13s 2ms/step - loss: 0.4912 - accuracy: 0.8597\n",
      "Epoch 12/135\n",
      "7651/7651 [==============================] - 15s 2ms/step - loss: 0.4723 - accuracy: 0.8637\n",
      "Epoch 13/135\n",
      "7651/7651 [==============================] - 12s 2ms/step - loss: 0.4542 - accuracy: 0.8680\n",
      "Epoch 14/135\n",
      "7651/7651 [==============================] - 13s 2ms/step - loss: 0.4372 - accuracy: 0.8730\n",
      "Epoch 15/135\n",
      "7651/7651 [==============================] - 13s 2ms/step - loss: 0.4241 - accuracy: 0.8751\n",
      "Epoch 16/135\n",
      "7651/7651 [==============================] - 12s 2ms/step - loss: 0.4123 - accuracy: 0.8785\n",
      "Epoch 17/135\n",
      "7651/7651 [==============================] - 15s 2ms/step - loss: 0.4042 - accuracy: 0.8812\n",
      "Epoch 18/135\n",
      "7651/7651 [==============================] - 13s 2ms/step - loss: 0.3914 - accuracy: 0.8845\n",
      "Epoch 19/135\n",
      "7651/7651 [==============================] - 15s 2ms/step - loss: 0.3823 - accuracy: 0.8864\n",
      "Epoch 20/135\n",
      "7651/7651 [==============================] - 13s 2ms/step - loss: 0.3738 - accuracy: 0.8885\n",
      "Epoch 21/135\n",
      "7651/7651 [==============================] - 13s 2ms/step - loss: 0.3654 - accuracy: 0.8912\n",
      "Epoch 22/135\n",
      "7651/7651 [==============================] - 13s 2ms/step - loss: 0.3577 - accuracy: 0.8931\n",
      "Epoch 23/135\n",
      "7651/7651 [==============================] - 12s 2ms/step - loss: 0.3499 - accuracy: 0.8957\n",
      "Epoch 24/135\n",
      "7651/7651 [==============================] - 12s 2ms/step - loss: 0.3416 - accuracy: 0.8971\n",
      "Epoch 25/135\n",
      "7651/7651 [==============================] - 12s 2ms/step - loss: 0.3374 - accuracy: 0.8976\n",
      "Epoch 26/135\n",
      "7651/7651 [==============================] - 13s 2ms/step - loss: 0.3336 - accuracy: 0.8991\n",
      "Epoch 27/135\n",
      "7651/7651 [==============================] - 12s 2ms/step - loss: 0.3255 - accuracy: 0.9016\n",
      "Epoch 28/135\n",
      "7651/7651 [==============================] - 12s 2ms/step - loss: 0.3182 - accuracy: 0.9034\n",
      "Epoch 29/135\n",
      "7651/7651 [==============================] - 13s 2ms/step - loss: 0.3180 - accuracy: 0.9036\n",
      "Epoch 30/135\n",
      "7651/7651 [==============================] - 13s 2ms/step - loss: 0.3119 - accuracy: 0.9055\n",
      "Epoch 31/135\n",
      "7651/7651 [==============================] - 13s 2ms/step - loss: 0.3055 - accuracy: 0.9074\n",
      "Epoch 32/135\n",
      "7651/7651 [==============================] - 12s 2ms/step - loss: 0.3038 - accuracy: 0.9070\n",
      "Epoch 33/135\n",
      "7651/7651 [==============================] - 13s 2ms/step - loss: 0.2993 - accuracy: 0.9081\n",
      "Epoch 34/135\n",
      "7651/7651 [==============================] - 13s 2ms/step - loss: 0.2978 - accuracy: 0.9092\n",
      "Epoch 35/135\n",
      "7651/7651 [==============================] - 13s 2ms/step - loss: 0.2910 - accuracy: 0.9105\n",
      "Epoch 36/135\n",
      "7651/7651 [==============================] - 13s 2ms/step - loss: 0.2863 - accuracy: 0.9117\n",
      "Epoch 37/135\n",
      "7651/7651 [==============================] - 12s 2ms/step - loss: 0.2854 - accuracy: 0.9123\n",
      "Epoch 38/135\n",
      "7651/7651 [==============================] - 12s 2ms/step - loss: 0.2820 - accuracy: 0.9136\n",
      "Epoch 39/135\n",
      "7651/7651 [==============================] - 13s 2ms/step - loss: 0.2766 - accuracy: 0.9149\n",
      "Epoch 40/135\n",
      "7651/7651 [==============================] - 12s 2ms/step - loss: 0.2773 - accuracy: 0.9146\n",
      "Epoch 41/135\n",
      "7651/7651 [==============================] - 12s 2ms/step - loss: 0.2725 - accuracy: 0.9156\n",
      "Epoch 42/135\n",
      "7651/7651 [==============================] - 13s 2ms/step - loss: 0.2727 - accuracy: 0.9153\n",
      "Epoch 43/135\n",
      "7651/7651 [==============================] - 13s 2ms/step - loss: 0.2672 - accuracy: 0.9178\n",
      "Epoch 44/135\n",
      "7651/7651 [==============================] - 11s 2ms/step - loss: 0.2649 - accuracy: 0.9178\n",
      "Epoch 45/135\n",
      "7651/7651 [==============================] - 11s 1ms/step - loss: 0.2605 - accuracy: 0.9194\n",
      "Epoch 46/135\n",
      "7651/7651 [==============================] - 12s 2ms/step - loss: 0.2598 - accuracy: 0.9191\n",
      "Epoch 47/135\n",
      "7651/7651 [==============================] - 12s 2ms/step - loss: 0.2596 - accuracy: 0.9193\n",
      "Epoch 48/135\n",
      "7651/7651 [==============================] - 12s 2ms/step - loss: 0.2537 - accuracy: 0.9208\n",
      "Epoch 49/135\n",
      "7651/7651 [==============================] - 12s 2ms/step - loss: 0.2529 - accuracy: 0.9213\n",
      "Epoch 50/135\n",
      "7651/7651 [==============================] - 12s 2ms/step - loss: 0.2515 - accuracy: 0.9215\n",
      "Epoch 51/135\n",
      "7651/7651 [==============================] - 12s 2ms/step - loss: 0.2481 - accuracy: 0.9227\n",
      "Epoch 52/135\n",
      "7651/7651 [==============================] - 13s 2ms/step - loss: 0.2475 - accuracy: 0.9234\n",
      "Epoch 53/135\n",
      "7651/7651 [==============================] - 13s 2ms/step - loss: 0.2460 - accuracy: 0.9229\n",
      "Epoch 54/135\n",
      "7651/7651 [==============================] - 12s 2ms/step - loss: 0.2423 - accuracy: 0.9238\n",
      "Epoch 55/135\n",
      "7651/7651 [==============================] - 13s 2ms/step - loss: 0.2394 - accuracy: 0.9249\n",
      "Epoch 56/135\n",
      "7651/7651 [==============================] - 13s 2ms/step - loss: 0.2415 - accuracy: 0.9244\n",
      "Epoch 57/135\n",
      "7651/7651 [==============================] - 12s 2ms/step - loss: 0.2371 - accuracy: 0.9259\n",
      "Epoch 58/135\n",
      "7651/7651 [==============================] - 12s 2ms/step - loss: 0.2347 - accuracy: 0.9258\n",
      "Epoch 59/135\n",
      "7651/7651 [==============================] - 12s 2ms/step - loss: 0.2317 - accuracy: 0.9268\n",
      "Epoch 60/135\n",
      "7651/7651 [==============================] - 12s 2ms/step - loss: 0.2336 - accuracy: 0.9264\n",
      "Epoch 61/135\n",
      "7651/7651 [==============================] - 12s 2ms/step - loss: 0.2286 - accuracy: 0.9277\n",
      "Epoch 62/135\n",
      "7651/7651 [==============================] - 12s 2ms/step - loss: 0.2313 - accuracy: 0.9277\n",
      "Epoch 63/135\n",
      "7651/7651 [==============================] - 12s 2ms/step - loss: 0.2261 - accuracy: 0.9289\n",
      "Epoch 64/135\n",
      "7651/7651 [==============================] - 12s 2ms/step - loss: 0.2289 - accuracy: 0.9281\n",
      "Epoch 65/135\n",
      "7651/7651 [==============================] - 12s 2ms/step - loss: 0.2249 - accuracy: 0.9291\n",
      "Epoch 66/135\n",
      "7651/7651 [==============================] - 13s 2ms/step - loss: 0.2221 - accuracy: 0.9299\n",
      "Epoch 67/135\n",
      "7651/7651 [==============================] - 12s 2ms/step - loss: 0.2198 - accuracy: 0.9308\n",
      "Epoch 68/135\n",
      "7651/7651 [==============================] - 12s 2ms/step - loss: 0.2214 - accuracy: 0.9302\n",
      "Epoch 69/135\n",
      "7651/7651 [==============================] - 12s 2ms/step - loss: 0.2207 - accuracy: 0.9309\n",
      "Epoch 70/135\n",
      "7651/7651 [==============================] - 12s 2ms/step - loss: 0.2181 - accuracy: 0.9313\n",
      "Epoch 71/135\n",
      "7651/7651 [==============================] - 12s 2ms/step - loss: 0.2162 - accuracy: 0.9320\n",
      "Epoch 72/135\n",
      "7651/7651 [==============================] - 12s 2ms/step - loss: 0.2173 - accuracy: 0.9313\n",
      "Epoch 73/135\n",
      "7651/7651 [==============================] - 12s 2ms/step - loss: 0.2145 - accuracy: 0.9325\n",
      "Epoch 74/135\n",
      "7651/7651 [==============================] - 12s 2ms/step - loss: 0.2129 - accuracy: 0.9326\n",
      "Epoch 75/135\n",
      "7651/7651 [==============================] - 12s 2ms/step - loss: 0.2114 - accuracy: 0.9331\n",
      "Epoch 76/135\n",
      "7651/7651 [==============================] - 13s 2ms/step - loss: 0.2103 - accuracy: 0.9337\n",
      "Epoch 77/135\n",
      "7651/7651 [==============================] - 13s 2ms/step - loss: 0.2101 - accuracy: 0.9332\n",
      "Epoch 78/135\n",
      "7651/7651 [==============================] - 12s 2ms/step - loss: 0.2068 - accuracy: 0.9346\n",
      "Epoch 79/135\n",
      "7651/7651 [==============================] - 12s 2ms/step - loss: 0.2090 - accuracy: 0.9338\n",
      "Epoch 80/135\n",
      "7651/7651 [==============================] - 12s 2ms/step - loss: 0.2060 - accuracy: 0.9348\n",
      "Epoch 81/135\n",
      "7651/7651 [==============================] - 12s 2ms/step - loss: 0.2039 - accuracy: 0.9357\n",
      "Epoch 82/135\n",
      "7651/7651 [==============================] - 12s 2ms/step - loss: 0.2071 - accuracy: 0.9344\n",
      "Epoch 83/135\n",
      "7651/7651 [==============================] - 12s 2ms/step - loss: 0.2034 - accuracy: 0.9353\n",
      "Epoch 84/135\n",
      "7651/7651 [==============================] - 12s 2ms/step - loss: 0.2025 - accuracy: 0.9363\n",
      "Epoch 85/135\n",
      "7651/7651 [==============================] - 12s 2ms/step - loss: 0.2015 - accuracy: 0.9365\n",
      "Epoch 86/135\n",
      "7651/7651 [==============================] - 12s 2ms/step - loss: 0.2016 - accuracy: 0.9359\n",
      "Epoch 87/135\n",
      "7651/7651 [==============================] - 12s 2ms/step - loss: 0.1988 - accuracy: 0.9369\n",
      "Epoch 88/135\n",
      "7651/7651 [==============================] - 12s 2ms/step - loss: 0.2005 - accuracy: 0.9365\n",
      "Epoch 89/135\n",
      "7651/7651 [==============================] - 12s 2ms/step - loss: 0.1977 - accuracy: 0.9367\n",
      "Epoch 90/135\n",
      "7651/7651 [==============================] - 12s 2ms/step - loss: 0.1985 - accuracy: 0.9368\n",
      "Epoch 91/135\n",
      "7651/7651 [==============================] - 12s 2ms/step - loss: 0.1996 - accuracy: 0.9369\n",
      "Epoch 92/135\n",
      "7651/7651 [==============================] - 12s 2ms/step - loss: 0.1951 - accuracy: 0.9384\n",
      "Epoch 93/135\n",
      "7651/7651 [==============================] - 12s 2ms/step - loss: 0.1936 - accuracy: 0.9388\n",
      "Epoch 94/135\n",
      "7651/7651 [==============================] - 12s 2ms/step - loss: 0.1925 - accuracy: 0.9389\n",
      "Epoch 95/135\n",
      "7651/7651 [==============================] - 12s 2ms/step - loss: 0.1937 - accuracy: 0.9382\n",
      "Epoch 96/135\n",
      "7651/7651 [==============================] - 12s 2ms/step - loss: 0.1915 - accuracy: 0.9391\n",
      "Epoch 97/135\n",
      "7651/7651 [==============================] - 12s 2ms/step - loss: 0.1915 - accuracy: 0.9388\n",
      "Epoch 98/135\n",
      "7651/7651 [==============================] - 12s 2ms/step - loss: 0.1885 - accuracy: 0.9400\n",
      "Epoch 99/135\n",
      "7651/7651 [==============================] - 12s 2ms/step - loss: 0.1916 - accuracy: 0.9391\n",
      "Epoch 100/135\n",
      "7651/7651 [==============================] - 12s 2ms/step - loss: 0.1897 - accuracy: 0.9402\n",
      "Epoch 101/135\n",
      "7651/7651 [==============================] - 12s 2ms/step - loss: 0.1880 - accuracy: 0.9403\n",
      "Epoch 102/135\n",
      "7651/7651 [==============================] - 12s 2ms/step - loss: 0.1876 - accuracy: 0.9408\n",
      "Epoch 103/135\n",
      "7651/7651 [==============================] - 12s 2ms/step - loss: 0.1877 - accuracy: 0.9406\n",
      "Epoch 104/135\n",
      "7651/7651 [==============================] - 13s 2ms/step - loss: 0.1841 - accuracy: 0.9413\n",
      "Epoch 105/135\n",
      "7651/7651 [==============================] - 12s 2ms/step - loss: 0.1863 - accuracy: 0.9411\n",
      "Epoch 106/135\n",
      "7651/7651 [==============================] - 11s 1ms/step - loss: 0.1862 - accuracy: 0.9413\n",
      "Epoch 107/135\n",
      "7651/7651 [==============================] - 11s 1ms/step - loss: 0.1825 - accuracy: 0.9423\n",
      "Epoch 108/135\n",
      "7651/7651 [==============================] - 11s 1ms/step - loss: 0.1857 - accuracy: 0.9414\n",
      "Epoch 109/135\n",
      "7651/7651 [==============================] - 11s 1ms/step - loss: 0.1846 - accuracy: 0.9421\n",
      "Epoch 110/135\n",
      "7651/7651 [==============================] - 11s 1ms/step - loss: 0.1843 - accuracy: 0.9417\n",
      "Epoch 111/135\n",
      "7651/7651 [==============================] - 11s 1ms/step - loss: 0.1809 - accuracy: 0.9423\n",
      "Epoch 112/135\n",
      "7651/7651 [==============================] - 11s 1ms/step - loss: 0.1830 - accuracy: 0.9421\n",
      "Epoch 113/135\n",
      "7651/7651 [==============================] - 11s 1ms/step - loss: 0.1842 - accuracy: 0.9415\n",
      "Epoch 114/135\n",
      "7651/7651 [==============================] - 11s 1ms/step - loss: 0.1819 - accuracy: 0.9426\n",
      "Epoch 115/135\n",
      "7651/7651 [==============================] - 11s 1ms/step - loss: 0.1798 - accuracy: 0.9433\n",
      "Epoch 116/135\n",
      "7651/7651 [==============================] - 11s 1ms/step - loss: 0.1794 - accuracy: 0.9430\n",
      "Epoch 117/135\n",
      "7651/7651 [==============================] - 11s 1ms/step - loss: 0.1826 - accuracy: 0.9423\n",
      "Epoch 118/135\n",
      "7651/7651 [==============================] - 11s 1ms/step - loss: 0.1774 - accuracy: 0.9438\n",
      "Epoch 119/135\n",
      "7651/7651 [==============================] - 11s 1ms/step - loss: 0.1743 - accuracy: 0.9445\n",
      "Epoch 120/135\n",
      "7651/7651 [==============================] - 11s 1ms/step - loss: 0.1800 - accuracy: 0.9435\n",
      "Epoch 121/135\n",
      "7651/7651 [==============================] - 11s 1ms/step - loss: 0.1781 - accuracy: 0.9435\n",
      "Epoch 122/135\n",
      "7651/7651 [==============================] - 11s 1ms/step - loss: 0.1757 - accuracy: 0.9446\n",
      "Epoch 123/135\n",
      "7651/7651 [==============================] - 11s 1ms/step - loss: 0.1758 - accuracy: 0.9446\n",
      "Epoch 124/135\n",
      "7651/7651 [==============================] - 11s 1ms/step - loss: 0.1759 - accuracy: 0.9445\n",
      "Epoch 125/135\n",
      "7651/7651 [==============================] - 11s 1ms/step - loss: 0.1769 - accuracy: 0.9443\n",
      "Epoch 126/135\n",
      "7651/7651 [==============================] - 11s 1ms/step - loss: 0.1708 - accuracy: 0.9459\n",
      "Epoch 127/135\n",
      "7651/7651 [==============================] - 11s 1ms/step - loss: 0.1734 - accuracy: 0.9456\n",
      "Epoch 128/135\n",
      "7651/7651 [==============================] - 11s 1ms/step - loss: 0.1727 - accuracy: 0.9461\n",
      "Epoch 129/135\n",
      "7651/7651 [==============================] - 11s 1ms/step - loss: 0.1731 - accuracy: 0.9451\n",
      "Epoch 130/135\n",
      "7651/7651 [==============================] - 11s 1ms/step - loss: 0.1748 - accuracy: 0.9449\n",
      "Epoch 131/135\n",
      "7651/7651 [==============================] - 11s 1ms/step - loss: 0.1709 - accuracy: 0.9456\n",
      "Epoch 132/135\n",
      "7651/7651 [==============================] - 11s 1ms/step - loss: 0.1703 - accuracy: 0.9470\n",
      "Epoch 133/135\n",
      "7651/7651 [==============================] - 11s 1ms/step - loss: 0.1705 - accuracy: 0.9461\n",
      "Epoch 134/135\n",
      "7651/7651 [==============================] - 11s 1ms/step - loss: 0.1701 - accuracy: 0.9462\n",
      "Epoch 135/135\n",
      "7651/7651 [==============================] - 11s 1ms/step - loss: 0.1686 - accuracy: 0.9465\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24c59a629d0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train_nums, epochs=135)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Save the model for later use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Technical note: pickle doesn't save models correctly, as it outputs this error when loading the pickle file: <br><br>\n",
    "FileNotFoundError: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for ram://0eb44777-6983-466e-ac15-adfa9d3dae07/variables/variables\n",
    " You may be trying to load on a different device from the computational device. Consider setting the `experimental_io_device` option in `tf.saved_model.LoadOptions` to the io_device such as '/job:localhost'. <br><br>\n",
    " That's why we are using keras's `save()` and `load_model()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: nnModel\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"nnModel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x24c6b4ce370>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.models.load_model(\"nnModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "25c1111f16a5ca6766f1c5cccb819e55c234e2449b101cca4e6a0a62df2327c3"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
